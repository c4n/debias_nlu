#!/bin/bash -l
#SBATCH --error=output/task.out.%j  # STDOUT output is written in slurm.out.JOBID
#SBATCH --output=output/task.out.%j # STDOUT error is written in slurm.err.JOBID
#SBATCH --job-name=fever_selfdistil       # Job name
#SBATCH --mem=64GB                  # Memory request for this job
#SBATCH --nodes=1                   # The number of nodes
#SBATCH --partition=gpu-cluster
#SBATCH --account=scads
#SBATCH --time=72:0:0                # Runing time 72 hours
#SBATCH --gpus=1                    # A number of GPUs  


# module load Anaconda3
# module load CUDA/10.1
# module load cuDNN/7
# module load HDF5


# TRANSFORMERS_OFFLINE=1
# HF_DATASETS_OFFLINE=1


MODEL_DIR=${1:-results/outputs_qqp_bert_base_1}
BATCH_SIZE=${2:-64}
N_DISTIL=${3:-0} ### [] Q0: Shall we set N_DISTIL = 3 or so ?

WEIGHTED_TRAINING_SET=${4:-data/paraphrase_identification/weighted_qqp.train.jsonl}
DEV_SET=${5:-data/paraphrase_identification/qqp.val.jsonl}
CONFIG=${6:-configs/paraphrase_identification/qqp_self_distil_1.jsonnet}

SUB_MODEL_DIR="$MODEL_DIR/base_distil"
mkdir -p $SUB_MODEL_DIR

# cd ~/debias_nlu
# source ~/envNLI/bin/activate

python3 -c "import torch; print('# Visible GPUs: ', torch.cuda.device_count());"

### [x] Q1: original dev set ?
### [] Q: scale from baseline model or reweighting model
### Todo: Create "pure" validation set from training set for Both FEVER and QQP (5k + stragify)
allennlp temp_scale $MODEL_DIR/model.tar.gz \
    $DEV_SET \
    --output-file $MODEL_DIR/temperature.pt \
    --cuda-device 0 \
    --include-package my_package 

### [x] Q2: original train set ?
### [x] Q3: CF_TYPE = Specific CF reader for each dataset e.g. "counterfactual_fever" ?
### [x?] Q4: Why both "cf_weight" and "entropy_curve" are zero ? Should I try to adjust them ?
### [x] Q5: Does it already supports QQP and FEVER ? (I sneaked into file and looks like it was already prepared)
allennlp cf_predict_scale $MODEL_DIR/model.tar.gz \
    $WEIGHTED_TRAINING_SET \
    $MODEL_DIR/temperature.pt \
    --output-file $SUB_MODEL_DIR/raw_train_result_0.jsonl \
    --predictor cf_textual_entailment_distill \
    --batch-size $BATCH_SIZE \
    --cuda-device 0 \
    --cf_weight  0 \
    --entropy_curve  0 \
    --include-package my_package \
    --silent


### [x] Q6: original dev set or challenge dev set ? (I could not find dev set for symmetric v0.1)
### [x] Q7: Does "distill_model2" was generated from the previous scripts ? Is it a typo ?
### [] Q: CF type in loop ??
# for i in `seq 0 $N_DISTIL`
# do
#     echo "Distillation #$i"
#     # Distill
#     ## Generate Training Set with Soft Target
#     python utils/create_distill_train_set.py \
#         -t $WEIGHTED_TRAINING_SET \
#         -p $SUB_MODEL_DIR/raw_train_result_${i}.jsonl \
#         -o $SUB_MODEL_DIR/distill_output_${i}.jsonl 
#     ## Train a distilled model
#     allennlp train $CONFIG \
#         -s $SUB_MODEL_DIR/distill_model_${i} \
#         --include-package my_package \
#         --overrides '{"train_data_path": "'"$SUB_MODEL_DIR/distill_output_${i}.jsonl"'",}'
#     # Temp Scale
#     allennlp temp_scale $SUB_MODEL_DIR/distill_model_${i}/model.tar.gz \
#         $DEV_SET \
#         --output-file $SUB_MODEL_DIR/distill_model_${i}/temperature.pt \
#         --cuda-device 0 \
#         --include-package my_package 
#     ## Predict
#     next_distill=$((i+1))
#     allennlp cf_predict_scale \
#         $SUB_MODEL_DIR/distill_model_${i} \
#         $WEIGHTED_TRAINING_SET  \
#         $SUB_MODEL_DIR/distill_model_${i}/temperature.pt \
#         --output-file $SUB_MODEL_DIR/raw_train_result_${next_distill}.jsonl \
#         --batch-size $BATCH_SIZE \
#         --predictor cf_textual_entailment_distill \
#         --cuda-device 0 \
#         --cf_weight  0.0  \
#         --entropy_curve  0 \
#         --include-package my_package \
#         --silent
# done
