#!/bin/bash -l
#SBATCH --error=/ist/users/canu/slurm_log/task.out.%j  # STDOUT output is written in slurm.out.JOBID
#SBATCH --output=/ist/users/canu/slurm_log/task.out.%j # STDOUT error is written in slurm.err.JOBID
#SBATCH --job-name=distill       # Job name
#SBATCH --mem=32GB                  # Memory request for this job
#SBATCH --nodes=1                   # The number of nodes
#SBATCH --partition=scads
#SBATCH --account=scads
#SBATCH --time=24:0:0                # Runing time 2 hours
#SBATCH --gpus=1                    # A number of GPUs  

# module load Anaconda3
 module load CUDA/10.1
 module load cuDNN/7
 module load HDF5

MODEL_DIR="/raid/can/nli_models/baseline/nli/outputs_bert_base_clark_1_seed33370"
# MODEL_DIR="/raid/can/nli_models/poe3/nli/outputs_poe_bert_base_korn_clark_1_seed33337"
weighted_training_set="/ist/ist-share/scads/can/korn_data/korn_lr_overlap_bias_probs.jsonl" #just to make sure that predictions' indices match with the training data

# mnli + mnli/snli hard + kaushik 
echo $MODEL_DIR
cd ~/debias_nlu
source ~/CI_env/bin/activate
mkdir -p $MODEL_DIR/normal
# Train Model
#Temp Scale
allennlp temp_scale $MODEL_DIR/model.tar.gz data/nli/multinli_1.0_dev_matched.jsonl   --output-file $MODEL_DIR/temperature.pt --cuda-device 0 --include-package my_package 
# Predict
if [ ! -f $MODEL_DIR/normal/raw_train_result_0.jsonl ]
then
   echo "File not found!"
   allennlp cf_predict_scale $MODEL_DIR/model.tar.gz $weighted_training_set $MODEL_DIR/temperature.pt --output-file $MODEL_DIR/normal/raw_train_result_0.jsonl --predictor cf_textual_entailment_distill --batch-size 64 --cuda-device 0 --cf_weight  0.0 --include-package my_package --silent
fi



echo $MODEL_DIR

max=0
for i in `seq 0 $max`
do
    # Distill
    ## Generate Training Set with Soft Target
    python utils/create_distill_train_set.py -t $weighted_training_set -p $MODEL_DIR/normal/raw_train_result_${i}.jsonl -o  $MODEL_DIR/normal/utama_distill_output_${i}.jsonl 
    ## Train a distilled model
    allennlp train configs/nli/knowledge_distill/mnli_bert_base_utama_distill_clark_1.jsonnet -s $MODEL_DIR/normal/utama_distill_model_${i} --include-package my_package --overrides '{"train_data_path": "'"$MODEL_DIR/normal/utama_distill_output_${i}.jsonl"'",}'
    # Temp Scale
    allennlp temp_scale $MODEL_DIR/normal/utama_distill_model_${i}/model.tar.gz data/nli/multinli_1.0_dev_matched.jsonl   --output-file $MODEL_DIR/normal/utama_distill_model_${i}/temperature.pt --cuda-device 0 --include-package my_package 
    ## Predict
    # next_distill=$((i+1))
    # allennlp cf_predict_scale $MODEL_DIR/normal/utama_distill_model_${i} $weighted_training_set $MODEL_DIR/normal/utama_distill_model_${i}/temperature.pt --output-file $MODEL_DIR/normal/raw_train_result_${next_distill}.jsonl --batch-size 64 --predictor cf_textual_entailment_distill --cuda-device 0 --cf_weight  0.0 --include-package my_package --silent
    # allennlp cf_predict_scale   $MODEL_DIR/normal/utama_distill_model_${i}  data/nli/multinli_1.0_train.jsonl --output-file $MODEL_DIR/normal/raw_train_result_${next_distill}.jsonl --batch-size 64 --cuda-device 0 --predictor textual_entailment --silent  
done
