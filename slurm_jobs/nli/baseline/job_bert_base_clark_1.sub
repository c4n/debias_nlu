#!/bin/bash -l
#SBATCH --error=/ist/users/canu/slurm_log/task.out.%j  # STDOUT output is written in slurm.out.JOBID
#SBATCH --output=/ist/users/canu/slurm_log/task.out.%j # STDOUT error is written in slurm.err.JOBID
#SBATCH --job-name=test_allennlp       # Job name
#SBATCH --mem=32GB                  # Memory request for this job
#SBATCH --nodes=1                   # The number of nodes
#SBATCH --partition=scads
#SBATCH --account=scads
#SBATCH --time=72:0:0                # Runing time 72 hours
#SBATCH --gpus=1                    # A number of GPUs  

 module load Anaconda3
 module load CUDA/11.0
 module load cuDNN/7
 module load HDF5

cd ~/debias_nlu
source ~/CI_env/bin/activate
allennlp train /ist/users/canu/debias_nlu/configs/nli/baseline/mnli_bert_base_clark_1.jsonnet -s /ist/ist-share/scads/can/outputs_bert_base_clark_1_seed23370/ --overrides '{ "random_seed": 23370, "numpy_seed": 2337, "pytorch_seed": 233}'
 
#psql training_strategy=cross_entropy
#psql debiasing_method=none
#psql exp_remarks=bert base baseline try to imitate clark et al setting batch size 32
