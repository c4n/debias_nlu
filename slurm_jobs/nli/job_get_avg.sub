#!/bin/bash -l
#SBATCH --error=/ist/users/canu/slurm_log/task.out.%j  # STDOUT output is written in slurm.out.JOBID
#SBATCH --output=/ist/users/canu/slurm_log/task.out.%j # STDOUT error is written in slurm.err.JOBID
#SBATCH --job-name=eval       # Job name
#SBATCH --mem=32GB                  # Memory request for this job
#SBATCH --nodes=1                   # The number of nodes
#SBATCH --partition=scads
#SBATCH --account=scads
#SBATCH --time=2:0:0                # Runing time 2 hours
#SBATCH --gpus=1                    # A number of GPUs  

# module load Anaconda3
 module load CUDA/10.1
 module load cuDNN/7
 module load HDF5

MODEL_DIR="/raid/can/nli_models/reweight_utama_github/nli/outputs_ps_bert_base_clark_utama_lex_seed33370"
# mnli + mnli/snli hard + kaushik 
echo $MODEL_DIR
cd ~/debias_nlu
source ~/CI_env/bin/activate


mkdir -p  $MODEL_DIR/normal

allennlp predict  $MODEL_DIR/model.tar.gz  /ist/users/canu/debias_nlu/data/nli/multinli_1.0_dev_mismatched.jsonl --output-file $MODEL_DIR/raw_embeddings.jsonl --batch-size 64 --cuda-device 0 --predictor textual_entailment --overrides '{"model": {"type": "get_avg_basic_classifier","text_field_embedder": {"token_embedders": {"tokens": {"type": "pretrained_transformer","model_name": "bert-base-uncased","max_length": 512}}},"seq2vec_encoder": {"type": "bert_pooler","pretrained_model": "bert-base-uncased",}, "feedforward": {"input_dim": 768,"num_layers": 1,"hidden_dims": 768, "activations": "tanh"}, "dropout": 0.1,"namespace": "tags"}}'
