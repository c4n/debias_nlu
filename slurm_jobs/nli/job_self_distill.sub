#!/bin/bash -l
#SBATCH --error=/ist/users/canu/slurm_log/task.out.%j  # STDOUT output is written in slurm.out.JOBID
#SBATCH --output=/ist/users/canu/slurm_log/task.out.%j # STDOUT error is written in slurm.err.JOBID
#SBATCH --job-name=eval       # Job name
#SBATCH --mem=16GB                  # Memory request for this job
#SBATCH --nodes=1                   # The number of nodes
#SBATCH --partition=gpu-cluster
#SBATCH --account=scads
#SBATCH --time=2:0:0                # Runing time 2 hours
#SBATCH --gpus=1                    # A number of GPUs  

# module load Anaconda3
 module load CUDA/10.1
 module load cuDNN/7
 module load HDF5

MODEL_DIR="/ist/ist-share/scads/can/outputs_ps_bert_base_clark_v2"
# mnli + mnli/snli hard + kaushik 
echo $MODEL_DIR
cd ~/debias_nlu
source ~/CI_env/bin/activate

# Train Model

# Predict
allennlp predict  $MODEL_DIR/model.tar.gz  data/nli/multinli_1.0_train_small.jsonl --output-file raw_train_result.jsonl --batch-size 64 --cuda-device 0 --predictor textual_entailment 
# allennlp cf_predict $MODEL_DIR/model.tar.gz data/nli/heuristics_evaluation_set.jsonl  --output-file test_cf_hans_overlap.jsonl --batch-size 64  --cf_type counterfactual_snli --cf_weight  2.1472015953718535  --predictor cf_textual_entailment --cuda-device 0 --include-package my_package # standard eval
echo $MODEL_DIR

# eval counterfactual

# hans
# allennlp cf_predict_scale $MODEL_DIR/model.tar.gz data/nli/heuristics_evaluation_set.jsonl $MODEL_DIR/temperature.pt --output-file $MODEL_DIR/cf_hans_result_mask_vxt214.jsonl --batch-size 64 --cuda-device 0 --cf_type counterfactual_snli --cf_weight  2.1472015953718535 --predictor cf_textual_entailment_entropy_scale --include-package my_package
# cd utils/
# python hans_parser.py -i $MODEL_DIR/cf_hans_result_mask_vxt214.jsonl -o $MODEL_DIR/cf_hans_mask_vxt214.out
# python evaluate_heur_output.py $MODEL_DIR/cf_hans_mask_vxt214.out > $MODEL_DIR/cf_hans_results_mask_vxt214.txt