{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d145a136",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b367e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee37b5a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>id</th>\n",
       "      <th>pair_label</th>\n",
       "      <th>lexical_overlap</th>\n",
       "      <th>word_swapping</th>\n",
       "      <th>hypo_len</th>\n",
       "      <th>overlapping score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>What are the best books for IBPS PO, SBI SO, S...</td>\n",
       "      <td>What are the best books for Bank P.O./IBPS pre...</td>\n",
       "      <td>296291</td>\n",
       "      <td>('What are the best books for IBPS PO, SBI SO,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Swap</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What are some of the results of The Congress o...</td>\n",
       "      <td>What were the results of the Congress of Vienna?</td>\n",
       "      <td>206762</td>\n",
       "      <td>('What are some of the results of The Congress...</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Swap</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Is our PM Modi doing the correct thing with 50...</td>\n",
       "      <td>What do you think about Modi's new policy on t...</td>\n",
       "      <td>397289</td>\n",
       "      <td>('Is our PM Modi doing the correct thing with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Swap</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>How can I use induction motor as generator?</td>\n",
       "      <td>How can we operate induction motor as a induct...</td>\n",
       "      <td>348835</td>\n",
       "      <td>('How can I use induction motor as generator?'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Swap</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>What are the differences between CRF and N-gra...</td>\n",
       "      <td>What is the difference between batch-mode and ...</td>\n",
       "      <td>137366</td>\n",
       "      <td>('What are the differences between CRF and N-g...</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Swap</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  is_duplicate  \\\n",
       "0           0             1   \n",
       "1           1             1   \n",
       "2           2             1   \n",
       "3           3             0   \n",
       "4           4             0   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  What are the best books for IBPS PO, SBI SO, S...   \n",
       "1  What are some of the results of The Congress o...   \n",
       "2  Is our PM Modi doing the correct thing with 50...   \n",
       "3        How can I use induction motor as generator?   \n",
       "4  What are the differences between CRF and N-gra...   \n",
       "\n",
       "                                           sentence2      id  \\\n",
       "0  What are the best books for Bank P.O./IBPS pre...  296291   \n",
       "1   What were the results of the Congress of Vienna?  206762   \n",
       "2  What do you think about Modi's new policy on t...  397289   \n",
       "3  How can we operate induction motor as a induct...  348835   \n",
       "4  What is the difference between batch-mode and ...  137366   \n",
       "\n",
       "                                          pair_label  lexical_overlap  \\\n",
       "0  ('What are the best books for IBPS PO, SBI SO,...                0   \n",
       "1  ('What are some of the results of The Congress...                1   \n",
       "2  ('Is our PM Modi doing the correct thing with ...                0   \n",
       "3  ('How can I use induction motor as generator?'...                0   \n",
       "4  ('What are the differences between CRF and N-g...                0   \n",
       "\n",
       "  word_swapping  hypo_len  overlapping score  \n",
       "0      Not Swap  0.033898           0.666667  \n",
       "1      Not Swap  0.033898           0.888889  \n",
       "2          Swap  0.072034           0.388889  \n",
       "3      Not Swap  0.038136           0.700000  \n",
       "4      Not Swap  0.042373           0.545455  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"qqp_train_overlapping_features.csv\") \n",
    "data_val = pd.read_csv(\"qqp_val_overlapping_features.csv\")\n",
    "data_dev = pd.read_csv(\"qqp_dev_overlapping_features.csv\")\n",
    "data_paws = pd.read_csv(\"qqp_paws_dev_and_test_overlapping_features.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a85387",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b095d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm, metrics, tree, decomposition, svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, OrthogonalMatchingPursuit\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdce078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_clfs_params(grid_size):\n",
    "    \"\"\"Define defaults for different classifiers.\n",
    "    Define three types of grids:\n",
    "    Test: for testing your code\n",
    "    Small: small grid\n",
    "    Large: Larger grid that has a lot more parameter sweeps\n",
    "    \"\"\"\n",
    "\n",
    "    clfs = {'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT': DecisionTreeClassifier(),\n",
    "        'SGD': SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=3) \n",
    "            }\n",
    "\n",
    "    large_grid = {'RF':{'n_estimators': [1,10,100,1000,10000], 'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10], 'n_jobs': [-1]},\n",
    "        'LR': {'penalty': ['l2'], 'C': [0.00001,0.0001,0.001,0.01,0.1,1,10], 'class_weight': ['balanced', None]},\n",
    "        'SGD': { 'loss': ['hinge','log','perceptron'], 'penalty': ['l2','l1','elasticnet']},\n",
    "        'ET': { 'n_estimators': [1,10,100,1000,10000], 'criterion' : ['gini', 'entropy'] ,'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10], 'n_jobs': [-1]},\n",
    "        'AB': { 'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1,10,100,1000,10000]},\n",
    "        'GB': {'n_estimators': [1,10,100,1000,10000], 'learning_rate' : [0.001,0.01,0.05,0.1,0.5],'subsample' : [0.1,0.5,1.0], 'max_depth': [1,3,5,10,20,50,100]},\n",
    "        'NB' : {},\n",
    "        'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1,5,10,20,50,100],'min_samples_split': [2,5,10]},\n",
    "        'SVM' :{'C' :[0.00001,0.0001,0.001,0.01,0.1,1,10],'kernel':['linear']},\n",
    "        'KNN' :{'n_neighbors': [1,5,10,25,50,100],'weights': ['uniform','distance'],'algorithm': ['auto','ball_tree','kd_tree']}\n",
    "           }\n",
    "    \n",
    "    small_grid = {'RF':{'n_estimators': [10,100], 'max_depth': [5,50], 'max_features': ['sqrt','log2'],'min_samples_split': [2,10], 'n_jobs': [-1]}, \n",
    "    'LR': {'penalty': ['l2'], 'C': [0.00001,0.001,0.1,1,10], 'class_weight': ['balanced', None]},\n",
    "    'SGD': {'loss': ['hinge','log','perceptron'], 'penalty': ['l2','l1','elasticnet']},\n",
    "    'ET': {'n_estimators': [10,100], 'criterion' : ['gini', 'entropy'] ,'max_depth': [5,50], 'max_features': ['sqrt','log2'],'min_samples_split': [2,10], 'n_jobs': [-1]},\n",
    "    'AB': {'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1,10,100,1000,10000]},\n",
    "    'GB': {'n_estimators': [10,100], 'learning_rate' : [0.001,0.1,0.5],'subsample' : [0.1,0.5,1.0], 'max_depth': [5,50]},\n",
    "    'NB' : {},\n",
    "    'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1,5,10,20,50,100],'min_samples_split': [2,5,10]},\n",
    "    'SVM' :{'C' :[0.00001,0.0001,0.001,0.01,0.1,1,10],'kernel':['linear']},\n",
    "    'KNN' :{'n_neighbors': [1,5,10,25,50,100],'weights': ['uniform','distance'],'algorithm': ['auto','ball_tree','kd_tree']}\n",
    "           }\n",
    "\n",
    "    gam_grid = {\n",
    "        'LR': {'penalty': ['l2'], 'C': [0.01], 'class_weight': ['balanced']},\n",
    "        'SGD': { 'loss': ['hinge','log','perceptron'], 'penalty': ['l2','l1','elasticnet']},\n",
    "        'AB': { 'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1,10,100,1000,10000]},\n",
    "        'GB': {'n_estimators': [1,10,100,1000,10000], 'learning_rate' : [0.001,0.01,0.05,0.1,0.5],'subsample' : [0.1,0.5,1.0], 'max_depth': [1,3,5,10,20,50,100]},\n",
    "        'NB' : {},\n",
    "        'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1,5,10,20,50,100],'min_samples_split': [2,5,10]},\n",
    "        'SVM' :{'C' :[0.00001,0.0001,0.001,0.01,0.1,1,10],'kernel':['linear']},\n",
    "           }\n",
    "    \n",
    "    test_grid = {'RF':{'n_estimators': [1], 'max_depth': [1], 'max_features': ['sqrt'],'min_samples_split': [10]}, \n",
    "    'LR': {'penalty': ['l2'], 'C': [0.01], 'class_weight': ['balanced', None]},\n",
    "    'SGD': {'loss': ['perceptron'], 'penalty': ['l1', 'elasticnet']},\n",
    "    'ET': {'n_estimators': [1], 'criterion' : ['gini'] ,'max_depth': [1], 'max_features': ['sqrt'],'min_samples_split': [10]},\n",
    "    'AB': {'algorithm': ['SAMME'], 'n_estimators': [1]},\n",
    "    'GB': {'n_estimators': [1], 'learning_rate' : [0.1],'subsample' : [0.5], 'max_depth': [1]},\n",
    "    'NB' : {},\n",
    "    'DT': {'criterion': ['gini'], 'max_depth': [1],'min_samples_split': [10]},\n",
    "    'SVM' :{'C' :[0.01],'kernel':['linear']},\n",
    "    'KNN' :{'n_neighbors': [5], 'weights': ['uniform'], 'algorithm': ['auto']}\n",
    "           }\n",
    "    \n",
    "    if (grid_size == 'gam'):\n",
    "        return clfs, gam_grid\n",
    "    elif (grid_size == 'small'):\n",
    "        return clfs, small_grid\n",
    "    elif (grid_size == 'test'):\n",
    "        return clfs, test_grid\n",
    "    else:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7767ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK = 0\n",
    "\n",
    "def clf_loop(models_to_run, clfs, grid, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Runs the loop using models_to_run, clfs, gridm and the data\n",
    "    \"\"\"\n",
    "    results_df =  pd.DataFrame(columns=('model_type', 'clf', 'parameters', 'auc-roc', 'acc', 'classification report', 'confusion matrix'))\n",
    "\n",
    "    for n in range(1, 2):\n",
    "        # create training and valdation sets\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        for index, clf in enumerate([clfs[x] for x in models_to_run]):\n",
    "            print(models_to_run[index])\n",
    "            parameter_values = grid[models_to_run[index]]\n",
    "            for p in ParameterGrid(parameter_values):\n",
    "                try:\n",
    "                    clf.set_params(**p)\n",
    "                    y_pred_probs = clf.fit(X_train, y_train).predict(X_test) # HERE\n",
    "                    roc_y_pred_probs = clf.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "                    confusion = pd.DataFrame(confusion_matrix(y_test, y_pred_probs, labels = [0, 1, 2]), index = [0, 1, 2], columns = [0, 1, 2])\n",
    "\n",
    "\n",
    "                    # you can also store the model, feature importances, and prediction scores\n",
    "                    # we're only storing the metrics for now\n",
    "                    y_pred_probs_sorted, y_test_sorted = zip(*sorted(zip(y_pred_probs, y_test), reverse=True))\n",
    "                    results_df.loc[len(results_df)] = [models_to_run[index], clf, p,\n",
    "                                                       roc_auc_score(y_test, roc_y_pred_probs),\n",
    "                                                       sklearn.metrics.accuracy_score(y_test, y_pred_probs),\n",
    "                                                       classification_report(y_test, y_pred_probs, output_dict=True),\n",
    "                                                       confusion.to_dict(orient=\"list\")]\n",
    "\n",
    "                    #print(results_df)\n",
    "                    if NOTEBOOK == 1:\n",
    "                        plot_precision_recall_n(y_test, y_pred_probs, clf)\n",
    "                except IndexError as e:\n",
    "                    print('Error:', e)\n",
    "                    continue\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a579530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features to use\n",
    "X_train = data.iloc[:, 9:].values\n",
    "X_val = data_val.iloc[:, 11:].values\n",
    "X_dev = data_dev.iloc[:, 11:].values\n",
    "X_paws = data_paws.iloc[:, 9:].values\n",
    "\n",
    "# define label\n",
    "y_train = data.iloc[:, 1].values\n",
    "y_val = data_val.iloc[:, 6].values\n",
    "y_dev = data_dev.iloc[:, 6].values\n",
    "y_paws = data_paws.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e8ea244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "  model_type                                                clf  \\\n",
      "0         LR  LogisticRegression(C=0.01, class_weight='balan...   \n",
      "\n",
      "                                          parameters   auc-roc     acc  \\\n",
      "0  {'C': 0.01, 'class_weight': 'balanced', 'penal...  0.739948  0.6794   \n",
      "\n",
      "                               classification report  \\\n",
      "0  {'0': {'precision': 0.7965583173996176, 'recal...   \n",
      "\n",
      "                                    confusion matrix  \n",
      "0  {0: [2083, 532, 0], 1: [1071, 1314, 0], 2: [0,...  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # define grid to use: test, small, large\n",
    "    grid_size = 'gam'\n",
    "    clfs, grid = define_clfs_params(grid_size)\n",
    "\n",
    "    # define models to run\n",
    "    models_to_run=['LR']\n",
    "\n",
    "    # call clf_loop and store results in results_df\n",
    "    results_df = clf_loop(models_to_run, clfs, grid, X_train, X_dev, y_train, y_dev)\n",
    "    print(results_df)\n",
    "    if NOTEBOOK == 1:\n",
    "        results_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b3cf25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression(penalty='l2', C=0.01, class_weight='balanced')\n",
    "lr_model = logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "501e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = lr_model.predict_proba(X_train).tolist()\n",
    "pred_val = lr_model.predict_proba(X_val).tolist()\n",
    "pred_dev = lr_model.predict_proba(X_dev).tolist()\n",
    "pred_paws = lr_model.predict_proba(X_paws).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e4e674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((399287, 4), (5000, 4), (5000, 4), (677, 4))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_df = pd.DataFrame()\n",
    "new_train_df[\"is_duplicate\"] = data[\"is_duplicate\"]\n",
    "new_train_df[\"sentence1\"] = data[\"sentence1\"]\n",
    "new_train_df[\"sentence2\"] = data[\"sentence2\"]\n",
    "new_train_df[\"bias_probs\"] = pred_train\n",
    "\n",
    "new_val_df = pd.DataFrame()\n",
    "new_val_df[\"is_duplicate\"] = data_val[\"is_duplicate\"]\n",
    "new_val_df[\"sentence1\"] = data_val[\"sentence1\"]\n",
    "new_val_df[\"sentence2\"] = data_val[\"sentence2\"]\n",
    "new_val_df[\"bias_probs\"] = pred_val\n",
    "\n",
    "new_dev_df = pd.DataFrame()\n",
    "new_dev_df[\"is_duplicate\"] = data_dev[\"is_duplicate\"]\n",
    "new_dev_df[\"sentence1\"] = data_dev[\"sentence1\"]\n",
    "new_dev_df[\"sentence2\"] = data_dev[\"sentence2\"]\n",
    "new_dev_df[\"bias_probs\"] = pred_dev\n",
    "\n",
    "new_paws_df = pd.DataFrame()\n",
    "new_paws_df[\"is_duplicate\"] = data_paws[\"is_duplicate\"]\n",
    "new_paws_df[\"sentence1\"] = data_paws[\"sentence1\"]\n",
    "new_paws_df[\"sentence2\"] = data_paws[\"sentence2\"]\n",
    "new_paws_df[\"bias_probs\"] = pred_paws\n",
    "\n",
    "new_train_df.shape, new_val_df.shape, new_dev_df.shape, new_paws_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deb62ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>bias_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What are the best books for IBPS PO, SBI SO, S...</td>\n",
       "      <td>What are the best books for Bank P.O./IBPS pre...</td>\n",
       "      <td>[0.3567053121189019, 0.6432946878810981]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What are some of the results of The Congress o...</td>\n",
       "      <td>What were the results of the Congress of Vienna?</td>\n",
       "      <td>[0.20908534776288057, 0.7909146522371194]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Is our PM Modi doing the correct thing with 50...</td>\n",
       "      <td>What do you think about Modi's new policy on t...</td>\n",
       "      <td>[0.5832793916113902, 0.4167206083886098]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>How can I use induction motor as generator?</td>\n",
       "      <td>How can we operate induction motor as a induct...</td>\n",
       "      <td>[0.33163352365953647, 0.6683664763404635]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>What are the differences between CRF and N-gra...</td>\n",
       "      <td>What is the difference between batch-mode and ...</td>\n",
       "      <td>[0.45372159187823935, 0.5462784081217607]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                          sentence1  \\\n",
       "0             1  What are the best books for IBPS PO, SBI SO, S...   \n",
       "1             1  What are some of the results of The Congress o...   \n",
       "2             1  Is our PM Modi doing the correct thing with 50...   \n",
       "3             0        How can I use induction motor as generator?   \n",
       "4             0  What are the differences between CRF and N-gra...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  What are the best books for Bank P.O./IBPS pre...   \n",
       "1   What were the results of the Congress of Vienna?   \n",
       "2  What do you think about Modi's new policy on t...   \n",
       "3  How can we operate induction motor as a induct...   \n",
       "4  What is the difference between batch-mode and ...   \n",
       "\n",
       "                                  bias_probs  \n",
       "0   [0.3567053121189019, 0.6432946878810981]  \n",
       "1  [0.20908534776288057, 0.7909146522371194]  \n",
       "2   [0.5832793916113902, 0.4167206083886098]  \n",
       "3  [0.33163352365953647, 0.6683664763404635]  \n",
       "4  [0.45372159187823935, 0.5462784081217607]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace nall to n\\/a\n",
    "new_train_df.sentence2 = new_train_df.sentence2.fillna('n\\/a')\n",
    "new_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "353573b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_json = new_train_df.to_json(orient='records', lines=True)\n",
    "with open('data/paraphrase_identification/train_prob_qqp_lr.jsonl', 'w') as json_file:\n",
    "    json_file.write(train_json)\n",
    "\n",
    "val_json = new_val_df.to_json(orient='records', lines=True)    \n",
    "with open('data/paraphrase_identification/val_prob_qqp_lr.jsonl', 'w') as json_file:\n",
    "    json_file.write(val_json)\n",
    "    \n",
    "dev_json = new_dev_df.to_json(orient='records', lines=True)    \n",
    "with open('data/paraphrase_identification/dev_prob_qqp_lr.jsonl', 'w') as json_file:\n",
    "    json_file.write(dev_json)\n",
    "    \n",
    "paws_json = new_paws_df.to_json(orient='records', lines=True)    \n",
    "with open('data/paraphrase_identification/paws_dev_and_test_prob_qqp_lr.jsonl', 'w') as json_file:\n",
    "    json_file.write(paws_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea875f",
   "metadata": {},
   "source": [
    "# Dataset maker reweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c02eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_weight(probs, label):\n",
    "    prob = probs[label]\n",
    "    return 1/prob\n",
    "\n",
    "def make_bias_prob(probs, label):\n",
    "    return probs[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cbd6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df['sample_weight'] = new_train_df[['bias_probs', 'is_duplicate']].apply(lambda x: make_sample_weight(*x), axis=1)\n",
    "\n",
    "new_train_df['bias_prob'] = new_paws_df[['bias_probs', 'is_duplicate']].apply(lambda x: make_bias_prob(*x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41eeebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>bias_probs</th>\n",
       "      <th>sample_weight</th>\n",
       "      <th>bias_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What were the major effects of the cambodia ea...</td>\n",
       "      <td>What were the major effects of the Iquique ear...</td>\n",
       "      <td>[0.1543575681999978, 0.8456424318000022]</td>\n",
       "      <td>6.478464</td>\n",
       "      <td>0.154358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy I 'm dating never texts me and I feel ...</td>\n",
       "      <td>The guy I 'm dating never wants me and I feel ...</td>\n",
       "      <td>[0.16584672552959534, 0.8341532744704047]</td>\n",
       "      <td>6.029664</td>\n",
       "      <td>0.165847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>How do I make my new phone number as group adm...</td>\n",
       "      <td>How do I make my old phone number as group adm...</td>\n",
       "      <td>[0.1543575681999978, 0.8456424318000022]</td>\n",
       "      <td>6.478464</td>\n",
       "      <td>0.154358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Why ca n't countries afford high quality produ...</td>\n",
       "      <td>Why ca n't countries afford China 's high qual...</td>\n",
       "      <td>[0.18940105962073162, 0.8105989403792684]</td>\n",
       "      <td>5.279802</td>\n",
       "      <td>0.189401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Do Mexican women like East Asian men ( Korean ...</td>\n",
       "      <td>Do East Asian women like Mexican men ( Korean ...</td>\n",
       "      <td>[0.1543575681999978, 0.8456424318000022]</td>\n",
       "      <td>6.478464</td>\n",
       "      <td>0.154358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate                                          sentence1  \\\n",
       "0             0  What were the major effects of the cambodia ea...   \n",
       "1             0  The guy I 'm dating never texts me and I feel ...   \n",
       "2             0  How do I make my new phone number as group adm...   \n",
       "3             0  Why ca n't countries afford high quality produ...   \n",
       "4             0  Do Mexican women like East Asian men ( Korean ...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  What were the major effects of the Iquique ear...   \n",
       "1  The guy I 'm dating never wants me and I feel ...   \n",
       "2  How do I make my old phone number as group adm...   \n",
       "3  Why ca n't countries afford China 's high qual...   \n",
       "4  Do East Asian women like Mexican men ( Korean ...   \n",
       "\n",
       "                                  bias_probs  sample_weight  bias_prob  \n",
       "0   [0.1543575681999978, 0.8456424318000022]       6.478464   0.154358  \n",
       "1  [0.16584672552959534, 0.8341532744704047]       6.029664   0.165847  \n",
       "2   [0.1543575681999978, 0.8456424318000022]       6.478464   0.154358  \n",
       "3  [0.18940105962073162, 0.8105989403792684]       5.279802   0.189401  \n",
       "4   [0.1543575681999978, 0.8456424318000022]       6.478464   0.154358  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_paws_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5557a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "paws_json = new_paws_df.to_json(orient='records', lines=True)\n",
    "with open('data/paraphrase_identification/paws_dev_and_test_overlap_only_bias_weighted.jsonl', 'w') as json_file:\n",
    "    json_file.write(paws_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb0920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
