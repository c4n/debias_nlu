{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Model for FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patomp/anaconda3/envs/envFakeNews/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from random import random\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from my_package.models.traditional.classifier import Classifier\n",
    "from my_package.utils.handcrafted_features.counter import count_negations\n",
    "from my_package.utils.handcrafted_features.overlap import get_lexical_overlap, get_entities_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_PREFIX = \"\" # \"sample_\" for example and \"\" for the real one\n",
    "\n",
    "TRAIN_DATA_FILE = \"../data/paraphrase_identification/%sqqp.train.jsonl\"%DUMMY_PREFIX\n",
    "DEV_DATA_FILE = \"../data/paraphrase_identification/%sqqp.dev.jsonl\"%DUMMY_PREFIX\n",
    "TEST_DATA_FILE = \"../data/paraphrase_identification/paws.dev_and_test.jsonl\"\n",
    "\n",
    "WEIGHT_KEY = \"sample_weight\"\n",
    "OUTPUT_TRAIN_DATA_FILE = \"../data/paraphrase_identification/%sweighted_qqp.train.jsonl\"%DUMMY_PREFIX\n",
    "SAVED_MODEL_PATH = \"../results/qqp/bias_model\"\n",
    "\n",
    "DOC1_KEY = \"sentence1\"\n",
    "DOC2_KEY = \"sentence2\"\n",
    "LABEL_KEY = \"is_duplicate\"\n",
    "\n",
    "POSSIBLE_LABELS = (\"0\", \"1\")\n",
    "BIAS_CLASS = \"1\"\n",
    "\n",
    "MAX_SAMPLE = -1 # -1 for non-maximal mode or a finite number e.g. 2000\n",
    "DROP_RATE = 0.0\n",
    "TEST_FRAC = 0.2\n",
    "\n",
    "MAX_TEST_SAMPLE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_prob_to_index(x: List[Dict[str, float]]) -> List[float]:\n",
    "    return [\n",
    "        x[\"0\"],\n",
    "        x[\"1\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(\n",
    "    file: str = TRAIN_DATA_FILE,\n",
    "    sent1_key: str = DOC1_KEY,\n",
    "    sent2_key: str = DOC2_KEY,\n",
    "    label_key: str = LABEL_KEY,\n",
    "    drop_rate: float = 0.0\n",
    "):\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    N_SAMPLE = 0\n",
    "\n",
    "    with open(file, 'r') as fh:\n",
    "        line = fh.readline()\n",
    "        while line:\n",
    "            if random() > drop_rate:\n",
    "                datapoint = json.loads(line)\n",
    "                docs.append([datapoint[sent1_key], datapoint[sent2_key]])\n",
    "                labels.append(str(datapoint[label_key]))\n",
    "\n",
    "                N_SAMPLE += 1\n",
    "                if MAX_SAMPLE != -1 and N_SAMPLE == MAX_SAMPLE:\n",
    "                    break\n",
    "            line = fh.readline()\n",
    "    print(\"# samples: \", N_SAMPLE)\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples:  2000\n"
     ]
    }
   ],
   "source": [
    "docs, labels = read_data(drop_rate=DROP_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['How can I stop my dog from chewing my shoes?',\n",
       "  'How do you stop a English Bulldog/Pitbull mix puppy from biting my shoes?'],\n",
       " ['What are the most interesting products and innovations that Guess is coming out with in 2016?',\n",
       "  'What are the most interesting products and innovations that Twitter is coming out with in 2016?']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, labels_train, labels_test = train_test_split(\n",
    "    docs, labels,\n",
    "    stratify=labels, test_size=TEST_FRAC,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = [\n",
    "    get_lexical_overlap,\n",
    "    get_entities_overlap\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_grams\": [1, 2],\n",
    "    \"top_ks\": [50, 50], # select by LMI\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(\n",
    "    possible_labels=POSSIBLE_LABELS,\n",
    "    feature_extractors=feature_extractors,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Top N-grams for sentence 1 ------\n",
      "1-gram LMI:  {'1': {'how': 0.0031326028947570458, 'do': 0.0022861665765403195, 'can': 0.0018635978552370462, 'you': 0.0010961121901843784, 'i': 0.001094121746698343, '500': 0.0010926629662799143, 'quora': 0.0008672703477951083, 'notes': 0.0008355657977434639, 'best': 0.000781103511897613, 'life': 0.0007218353413456668, '1000': 0.0007172359880179036, 'money': 0.0007016123840321265, 'your': 0.0006490025244530314, 'improve': 0.000640805035180602, 'war': 0.00063930085172901, 'who': 0.0006348082060698078, 'think': 0.0006036014370502948, '2000': 0.0005784686292070134, 'rupee': 0.0005784686292070134, 'rs': 0.000557724477531417, 'ever': 0.0005437259735849995, 'really': 0.0005251035191321394, 'english': 0.0005150140500252845, 'is': 0.00048535574681406196, 'battle': 0.00044992004493878823, 'my': 0.00041669792055320105, 'ways': 0.0004137301347816252, 'india': 0.0004038868569796261, 'president': 0.0003922159970314348, 'currency': 0.00038564575280467564, 'get': 0.00038271239329870253, 'donald': 0.0003782352792421134, 'trump': 0.00035714584860245253, 'make': 0.0003513024595206068, 'what': 0.000343753018242375, '2017': 0.00033359419790774563, 'will': 0.0003264868990306765, 'way': 0.0003231014903165577, 'win': 0.00032137146067056304, 'note': 0.00032137146067056304, 'could': 0.00031194959312787885, 'on': 0.0003118719780868519, 'hillary': 0.0003094099088832349, 'one': 0.0003000969774068851, 'time': 0.0002981909444220547, 'lose': 0.0002885050901500138, 'learning': 0.0002885050901500138, 'possible': 0.00028152175741946766, 'weight': 0.00027186298679249976, 'period': 0.0002700681918332749}, '0': {'in': 0.0011307706136524367, 'to': 0.0006535215011352207, 'it': 0.000630299725330238, 'does': 0.0006077001562428417, 'and': 0.0005772255182477306, 'the': 0.0005571186181729015, 'or': 0.0005258173536561602, 'not': 0.0005153376762384283, 'of': 0.0005005317609592479, 'a': 0.00043121209103698344, 'for': 0.00038858856645300667, 'mean': 0.0003468012943622186, 'like': 0.0003385945264474605, 'day': 0.00032451056582662104, 'someone': 0.00029650291500520846, 'when': 0.00028220717523936607, 'they': 0.00027919984591331725, 'at': 0.0002645284729204772, 'them': 0.00025960845266129683, 'where': 0.00024991801150427986, 'from': 0.0002449767275698998, 'products': 0.0002379744149395221, 'between': 0.00023519489458637496, 'as': 0.00023383826084711248, 'out': 0.00021885199503985254, 'much': 0.00019754138393159393, 'different': 0.0001947063394959726, 'email': 0.0001947063394959726, 'messenger': 0.0001947063394959726, 'years': 0.0001947063394959726, 'me': 0.00018979169122711084, 'want': 0.00017654830571715047, 'why': 0.00016984882701627976, 'now': 0.00016270191708812564, 'used': 0.00015550534286743674, 'science': 0.00015550534286743674, 'watch': 0.00015143826405242315, 'california': 0.00015143826405242315, 'youtube': 0.00015143826405242315, 'working': 0.00015143826405242315, 'say': 0.00015143826405242315, 'coming': 0.00015143826405242315, 'current': 0.00015143826405242315, 'price': 0.00015143826405242315, 'majors': 0.00015143826405242315, 'true': 0.00015143826405242315, 'employees': 0.00015143826405242315, 'engineer': 0.00015143826405242315, 'run': 0.00015143826405242315, 'his': 0.00015143826405242315}} \n",
      "\n",
      "2-gram LMI:  {'1': {'how_can': 0.00218389897017682, 'can_i': 0.0015882665340304321, 'how_do': 0.0013885866473988212, 'do_i': 0.0010774198385392702, '500_and': 0.0009286549495086499, 'the_best': 0.0008645052719602708, 'do_you': 0.0007712847012364777, 'improve_my': 0.0006015900150945994, 'and_1000': 0.0005714799689283999, 'on_quora': 0.0005562995936116628, 'what_is': 0.0005137871210290968, 'you_have': 0.000461074352593829, 'i_make': 0.000461074352593829, 'i_improve': 0.000461074352593829, 'rupee_notes': 0.00042860997669629994, 'i_get': 0.0004284915006869321, 'donald_trump': 0.00042366802087616845, 'ways_to': 0.00039124411945306106, 'what_should': 0.0003861736663604688, 'you_think': 0.0003714076711137933, 'battle_of': 0.00035717498058024995, '1000_rupee': 0.00035717498058024995, 'rs_500': 0.00035717498058024995, 'do_to': 0.0003456306857462898, 'and_how': 0.00033126334908464914, 'will_be': 0.0003218567801652414, 'why_do': 0.00031475216874424916, 'make_money': 0.0003007950075472997, 'of_the': 0.00029079487730902874, 'people_think': 0.00028573998446419996, 'the_battle': 0.00028573998446419996, 'currency_notes': 0.00028573998446419996, 'world_war': 0.00028573998446419996, 'lose_weight': 0.00028573998446419996, 'for_2017': 0.00028573998446419996, 'views_on': 0.00028573998446419996, 'one_thing': 0.00028573998446419996, 'know_about': 0.00028573998446419996, 'i_do': 0.00028566100045795473, 'way_to': 0.00028338863056485665, 'be_a': 0.0002666987235001594, 'of_life': 0.00025312641956187754, 'get_rid': 0.00025312641956187754, 'to_improve': 0.00025312641956187754, 'in_india': 0.0002387146125036873, 'possible_to': 0.00023204605767575742, 'a_blog': 0.0002305371762969145, 'things_that': 0.0002305371762969145, 'if_the': 0.0002305371762969145, \"n't_know\": 0.0002305371762969145}, '0': {'what_does': 0.00039647912600721533, 'does_it': 0.0003733136512484247, 'if_i': 0.00032705294273736277, 'in_the': 0.00031342828793315376, 'how_much': 0.0002951865550761574, 'as_a': 0.00025743463671787126, 'where_can': 0.00022015308585636786, 'it_mean': 0.00021062833913280377, 'are_the': 0.0002095592023970386, 'i_have': 0.00018722519034027, 'why_are': 0.00016643317434539162, 'i_want': 0.00016382204154773627, 'for_a': 0.00015581324792534907, 'do_people': 0.00015198249579552643, 'would_you': 0.000143785373850976, 'the_top': 0.0001404188927552025, 'between_the': 0.0001404188927552025, 'in_chennai': 0.0001404188927552025, 'why_is': 0.00013899109384396316, 'the_difference': 0.00013466278489657598, 'the_world': 0.00012894921252772329, 'much_does': 0.00012894921252772329, 'did_the': 0.00011701574396266876, 'to_have': 0.00011701574396266876, 'to_use': 0.00011701574396266876, 'able_to': 0.00011701574396266876, 'definition_of': 0.00011701574396266876, 'most_interesting': 0.00011701574396266876, 'in_2016': 0.00011701574396266876, 'as_an': 0.00011701574396266876, 'or_a': 0.00011701574396266876, 'or_not': 0.00011701574396266876, 'new_employees': 0.00011701574396266876, 'what_kind': 0.00011701574396266876, 'to_be': 0.00011522936100854853, 'i_am': 0.00011522936100854853, 'difference_between': 0.00010965553385880031, 'need_to': 0.00010601266877442572, 'it_be': 0.00010601266877442572, 'of_a': 0.00010601266877442572, 'in_my': 9.893087737515884e-05, 'want_to': 9.893087737515884e-05, 'of_your': 9.3612595170135e-05, 'are_particularly': 9.3612595170135e-05, 'and_moral': 9.3612595170135e-05, 'me_to': 9.3612595170135e-05, 'be_able': 9.3612595170135e-05, 'of_using': 9.3612595170135e-05, 'the_definition': 9.3612595170135e-05, 'the_differences': 9.3612595170135e-05}} \n",
      "\n",
      "------ Top N-grams for sentence 2 ------\n",
      "1-gram LMI:  {'1': {'how': 0.002700432183084882, 'the': 0.0019517619239862263, 'can': 0.001626415887028363, 'do': 0.0013762738519139943, 'what': 0.0011359772404782042, 'notes': 0.000978366508906637, 'will': 0.0009280345098137749, 'quora': 0.0009266422589787771, 'best': 0.0008491142881026005, '1000': 0.0008479176410524187, '500': 0.000793824230721235, 'are': 0.0006979207422205941, 'really': 0.0006963560609223134, 'why': 0.0006640489192475123, 'you': 0.0006271975120146985, 'improve': 0.0006183631275251608, 'money': 0.0006183631275251608, 'rupee': 0.0005870199053439821, 'life': 0.0005614376509874772, 'trump': 0.0005578477609760019, 'english': 0.0005334775563134011, '2000': 0.000521795471416873, 'war': 0.000509499422886232, 'ways': 0.00047307007671872444, 'your': 0.00047261127729430883, 'ban': 0.00045657103748976397, 'we': 0.00044719257667556224, 'donald': 0.00042458285240519337, 'be': 0.0003993045826048998, 'battle': 0.0003913466035626548, 'increase': 0.0003787505613130957, 'on': 0.00037401820091630263, \"n't\": 0.0003691611963017516, 'make': 0.0003664236863473857, 'rs': 0.00036374675708394, 'win': 0.0003577950128987285, 'question': 0.0003577950128987285, 'did': 0.0003450104885859027, 'america': 0.0003261221696355457, 'economy': 0.0003261221696355457, 'note': 0.0003261221696355457, 'ever': 0.00032221252293076634, 'india': 0.00031682747834436655, 'under': 0.00031638436665477654, 'think': 0.0002962687933769942, 'blog': 0.0002944295662403255, '2017': 0.0002944295662403255, 'book': 0.00027493700489833223, 'created': 0.00027493700489833223, 'writing': 0.00027493700489833223}, '0': {'a': 0.0020950599017973363, 'to': 0.0014227129511332859, 'does': 0.0008912217654402759, 'an': 0.0006121835993966012, 'from': 0.0005698154277388839, 'not': 0.0005551875138396917, 'for': 0.0005113885672712287, 'as': 0.00048778917124385956, 'much': 0.0004816830087999399, 'in': 0.0004518746565734025, 'should': 0.0004242003752318813, 'they': 0.00042147309611270174, 'or': 0.0004053131819337471, 'have': 0.0003614526609113972, 'mean': 0.0003457385264391854, 'at': 0.0002925691294073163, 'it': 0.00028405905201460017, 'and': 0.00027381612197360225, 'which': 0.00023395454865864776, 'college': 0.00023184471539259104, 'someone': 0.0002307327156450171, 'person': 0.00021990488967824184, 'so': 0.00021071486154249287, 'many': 0.00020514423766248663, 'year': 0.00020514423766248663, 'first': 0.00019899043009952397, 'most': 0.00019627094695232514, 'friend': 0.0001896911307757563, 'employees': 0.0001896911307757563, 'different': 0.0001896911307757563, '5': 0.00017810383123279723, 'no': 0.00017810383123279723, 'body': 0.00017810383123279723, 'like': 0.0001731191198522732, 'old': 0.0001695451294687883, 'police': 0.00016861433846733894, 'play': 0.00016861433846733894, 'using': 0.00016861433846733894, 'web': 0.00016861433846733894, 'am': 0.00016579132578958743, 'job': 0.00016259639041461984, 'but': 0.00016259639041461984, 'his': 0.00015725294284592268, 'way': 0.00015197077592526655, 'us': 0.0001512349004377739, 'into': 0.00014905805133227725, 'foreign': 0.00014753754615892158, 'house': 0.00014753754615892158, 'women': 0.00014753754615892158, 'home': 0.00014753754615892158}} \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram LMI:  {'1': {'how_do': 0.0016769479957734275, 'how_can': 0.0016401141655068615, 'do_i': 0.0014621707038709555, 'what_is': 0.0008920248299538186, 'can_i': 0.0008532672839026691, 'what_are': 0.0007200096490867508, 'the_best': 0.0007022899896785075, 'on_quora': 0.0006349114134642144, 'how_did': 0.0005442978836141374, 'is_the': 0.0005244353137581608, 'will_the': 0.0005226440803104914, '500_and': 0.0005081760779579652, 'and_1000': 0.0005081760779579652, 'ways_to': 0.00047583312478553504, 'donald_trump': 0.00047583312478553504, 'i_get': 0.00044957676256034487, 'are_the': 0.0004392461393483449, 'are_some': 0.00043922357199322376, '1000_notes': 0.00043557949539254167, 'the_ban': 0.00043557949539254167, 'can_we': 0.0004236085736049264, 'make_money': 0.0003996504149125401, 'do_you': 0.0003977486430961743, 'the_indian': 0.0003785218717673998, 'can_one': 0.00036298291282711807, 'ban_of': 0.00036298291282711807, 'rupee_notes': 0.00036298291282711807, 'some_good': 0.000354152351395897, 'improve_my': 0.000354152351395897, 'your_life': 0.00030674554182343066, 'be_a': 0.0003045896683282389, 'a_blog': 0.0002903863302616944, 'the_battle': 0.0002903863302616944, 'battle_of': 0.0002903863302616944, '1000_rupee': 0.0002903863302616944, 'the_usa': 0.0002903863302616944, 'world_war': 0.0002903863302616944, 'when_will': 0.0002903863302616944, 'of_500': 0.0002903863302616944, 'indian_economy': 0.0002903863302616944, 'rs_2000': 0.0002903863302616944, 'will_be': 0.00028549987487132104, 'be_the': 0.00028549987487132104, 'you_think': 0.00028549987487132104, 'and_the': 0.0002591975221358124, 'like_to': 0.0002465015347141984, 'i_improve': 0.0002465015347141984, 'ever_seen': 0.000235323146717164, 'i_be': 0.000235323146717164, 'why_has': 0.000235323146717164}, '0': {'is_a': 0.00043838605900662546, 'the_most': 0.0004321536058122298, 'how_much': 0.0003821066665109589, 'should_i': 0.0003801566292536548, 'as_a': 0.0003265721000179624, 'i_have': 0.0003183074168885733, 'what_does': 0.0003044716592554401, 'if_i': 0.0002601523627058819, 'for_a': 0.0002152907562560859, 'start_a': 0.00020462619657122568, 'in_my': 0.00020462619657122568, 'way_to': 0.0002007766030466639, 'i_am': 0.00019415596233651503, 'there_a': 0.00016856529084794325, 'how_long': 0.00016856529084794325, 'from_a': 0.00015915370844428665, 'will_i': 0.00015915370844428665, 'and_a': 0.00015915370844428665, 'have_a': 0.00015915370844428665, 'how_many': 0.00015816493748669843, 'i_start': 0.00014612868633554182, 'a_good': 0.00013673687524135862, 'year_old': 0.00013641746438081714, 'it_mean': 0.00013641746438081714, 'to_my': 0.00013641746438081714, 'some_things': 0.00013641746438081714, 'does_my': 0.00013641746438081714, 'long_does': 0.00013641746438081714, 'how_to': 0.00013641746438081714, 'for_the': 0.00012904978219480277, 'how_is': 0.00012904978219480277, 'have_to': 0.00012376137229485874, 'in_2016': 0.00012376137229485874, 'does_it': 0.00011612164826863678, 'to_work': 0.00011368122031734762, 'i_want': 0.00011368122031734762, 'a_startup': 0.00011368122031734762, 'it_normal': 0.00011368122031734762, 'to_see': 0.00011368122031734762, 'most_interesting': 0.00011368122031734762, 'coming_out': 0.00011368122031734762, 'are_good': 0.00011368122031734762, 'for_me': 0.00011368122031734762, 'number_of': 0.00011368122031734762, 'first_day': 0.00011368122031734762, 'mean_when': 0.00011368122031734762, 'it_take': 0.00011368122031734762, 'i_delete': 0.00011368122031734762, 'to_learn': 0.00010764537812804295, 'an_android': 0.00010149055308514671}} \n",
      "\n",
      "n_features: 402\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(docs_train, labels_train)\n",
    "classifier.save(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 0.9326622042463655, '1': 0.06733779575363455},\n",
       " {'0': 0.9184946160216139, '1': 0.0815053839783861}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inferential Examples\n",
    "x = [['Roman Atwood is a content creator .',\n",
    "  'He is best known for his vlogs , where he posts updates about his life on a daily basis .'],\n",
    " ['Roman Atwood is a content creator .',\n",
    "  \"He also has another YouTube channel called `` RomanAtwood '' , where he posts pranks .\"]] \n",
    "classifier.inference(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original train set: 0.682\n",
      "F1-pos on original train set: 0.506\n",
      "F1-neg on original train set: 0.766\n"
     ]
    }
   ],
   "source": [
    "y_preds = classifier.predict(docs_test)\n",
    "\n",
    "print(\"Accuracy on original train set: %.3f\"% accuracy_score(labels_test, y_preds))\n",
    "print(\"F1-pos on original train set: %.3f\"% f1_score(labels_test, y_preds, pos_label='1'))\n",
    "print(\"F1-neg on original train set: %.3f\"% f1_score(labels_test, y_preds, pos_label='0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original train set: 0.682\n",
      "F1-pos on original train set: 0.506\n",
      "F1-neg on original train set: 0.766\n"
     ]
    }
   ],
   "source": [
    "# validate load process of the classifier\n",
    "test_classifier = Classifier(\n",
    "    possible_labels=POSSIBLE_LABELS,\n",
    "    feature_extractors=feature_extractors,\n",
    "    config=config\n",
    ")\n",
    "test_classifier.load(SAVED_MODEL_PATH)\n",
    "\n",
    "y_preds = classifier.predict(docs_test)\n",
    "\n",
    "print(\"Accuracy on original train set: %.3f\"% accuracy_score(labels_test, y_preds))\n",
    "print(\"F1-pos on original train set: %.3f\"% f1_score(labels_test, y_preds, pos_label='1'))\n",
    "print(\"F1-neg on original train set: %.3f\"% f1_score(labels_test, y_preds, pos_label='0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples:  200\n"
     ]
    }
   ],
   "source": [
    "eval_docs, eval_labels = read_data(DEV_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original test set: 0.705\n",
      "F1-pos on original test set: 0.550\n",
      "F1-neg on original test set: 0.781\n"
     ]
    }
   ],
   "source": [
    "y_preds = classifier.predict(eval_docs)\n",
    "\n",
    "print(\"Accuracy on original test set: %.3f\"% accuracy_score(eval_labels, y_preds))\n",
    "print(\"F1-pos on original test set: %.3f\"% f1_score(eval_labels, y_preds, pos_label='1'))\n",
    "print(\"F1-neg on original test set: %.3f\"% f1_score(eval_labels, y_preds, pos_label='0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On symmetric (challenge) test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples:  677\n"
     ]
    }
   ],
   "source": [
    "eval_docs, eval_labels = read_data(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on challenge test set: 0.409\n",
      "F1-pos on challenge test set: 0.419\n",
      "F1-neg on challenge test set: 0.399\n"
     ]
    }
   ],
   "source": [
    "y_preds = classifier.predict(eval_docs)\n",
    "\n",
    "print(\"Accuracy on challenge test set: %.3f\"% accuracy_score(eval_labels, y_preds))\n",
    "print(\"F1-pos on challenge test set: %.3f\"% f1_score(eval_labels, y_preds, pos_label='1'))\n",
    "print(\"F1-neg on challenge test set: %.3f\"% f1_score(eval_labels, y_preds, pos_label='0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predicted probability to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(prob_score_bias_class: float, ground_truth_label: str, bias_label: str = BIAS_CLASS) -> float:\n",
    "    assert type(ground_truth_label) == type(bias_label)\n",
    "    if ground_truth_label == bias_label:\n",
    "        return 1/prob_score_bias_class\n",
    "    return 1/(1-prob_score_bias_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cases\n",
    "def test_bias_label():\n",
    "    weight = get_weight(0.2, \"A\", bias_label=\"A\")\n",
    "    np.testing.assert_almost_equal(1/0.2, weight, 5)\n",
    "    \n",
    "def test_not_bias_label():\n",
    "    weight = get_weight(0.2, \"B\", bias_label=\"A\")\n",
    "    np.testing.assert_almost_equal(1/0.8, weight, 5)\n",
    "    \n",
    "test_bias_label()\n",
    "test_not_bias_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_output = open(OUTPUT_TRAIN_DATA_FILE, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 0\n",
    "\n",
    "with open(TRAIN_DATA_FILE, 'r') as fh:\n",
    "    line = fh.readline()\n",
    "    while line:\n",
    "        datapoint = json.loads(line)\n",
    "        x = [[datapoint[DOC1_KEY], datapoint[DOC2_KEY]]]\n",
    "        \n",
    "        probs = classifier.inference(x)[0]\n",
    "        prob = probs[BIAS_CLASS]\n",
    "        weight = get_weight(\n",
    "            prob_score_bias_class=prob,\n",
    "            ground_truth_label=str(datapoint[LABEL_KEY]),\n",
    "            bias_label=BIAS_CLASS\n",
    "        )\n",
    "        if datapoint.get(\"weight\", None) != None:\n",
    "            del datapoint[\"weight\"] # only for fever\n",
    "        f_output.write(\"%s\\n\"%json.dumps({\n",
    "            **datapoint,\n",
    "            WEIGHT_KEY: weight,\n",
    "            \"bias_probs\": inference_prob_to_index(probs),\n",
    "            \"bias_prob\": prob\n",
    "        }))\n",
    "\n",
    "        N_SAMPLE += 1\n",
    "        if MAX_SAMPLE != -1 and N_SAMPLE == MAX_SAMPLE:\n",
    "            break\n",
    "        line = fh.readline()\n",
    "        \n",
    "f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
