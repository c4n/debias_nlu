{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civil-verification",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "overhead-marina",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-26 18:45:49.725757: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "import torch\n",
    "from allennlp.data import Token, Vocabulary, TokenIndexer, Tokenizer\n",
    "from allennlp.data.fields import ListField, TextField, Field\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedTransformerIndexer,\n",
    "    PretrainedTransformerMismatchedIndexer,\n",
    ")\n",
    "from allennlp.data.tokenizers import (\n",
    "    CharacterTokenizer,\n",
    "    PretrainedTransformerTokenizer,\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "\n",
    "\n",
    "from allennlp.common.params import Params\n",
    "from my_package import my_readers, my_classifiers, my_trainers, my_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155ac82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_package.data.dataset_readers.counterfactual_reader import  CounterfactualSnliReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5f70e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import allennlp\n",
    "allennlp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d8c1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "systematic-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_package.data.fields.float_fields import FloatField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inner-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=FloatField(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-timing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unlike-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_package.my_readers import OverlapSnliReader, ReversibleSnliReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "synthetic-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/ist/users/canu/debias_nlu/data/nli/multinli_1.0_train.jsonl\"\n",
    "validation_data_path = \"/ist/users/canu/debias_nlu/data/nli/multinli_1.0_dev_matched.jsonl\"\n",
    "test_data_path = \"/ist/users/canu/debias_nlu/data/nli/multinli_1.0_dev_mismatched.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "democratic-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models.pair_classification.dataset_readers import snli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "diverse-budapest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/config.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/vocab.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/merges.txt\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/tokenizer.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/added_tokens.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/config.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/vocab.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/merges.txt\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/tokenizer.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/added_tokens.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "transformer_model = \"roberta-large\"\n",
    "transformer_dim = 1024\n",
    "pretrained_transformer_tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model,add_special_tokens = False)\n",
    "token_indexer  = PretrainedTransformerIndexer(model_name=transformer_model,max_length=512 )\n",
    "reader = OverlapSnliReader(tokenizer=pretrained_transformer_tokenizer,token_indexers={\"tokens\":token_indexer})\n",
    "cf_reader = CounterfactualSnliReader(tokenizer=pretrained_transformer_tokenizer,token_indexers={\"tokens\":token_indexer})\n",
    "# adversarial_reader = ReversibleSnliReader(tokenizer=pretrained_transformer_tokenizer,token_indexers={\"tokens\":token_indexer}, reversible = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "speaking-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate and use the dataset reader to read a file containing the data\n",
    "train_dataset = reader.read(train_data_path)\n",
    "train_dataset_list = list(train_dataset)\n",
    "\n",
    "validation_dataset = reader.read(validation_data_path)\n",
    "validation_dataset_list = list(validation_dataset)\n",
    "                               \n",
    "counter_factual_val =cf_reader.read(validation_data_path)  \n",
    "cf_dataset_list = list(counter_factual_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sharp-newport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of its first element:  <class 'allennlp.data.instance.Instance'>\n",
      "size of training dataset:  392702\n"
     ]
    }
   ],
   "source": [
    "print(\"type of its first element: \", type(train_dataset_list[0]))\n",
    "print(\"size of training dataset: \", len(train_dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "first-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': <allennlp.data.fields.text_field.TextField at 0x7fdcf2777240>,\n",
       " 'label': <allennlp.data.fields.label_field.LabelField at 0x7fdc7f212fc0>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset_list[0].fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "after-contrary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3773110c58fa46ecb83313f192ecb589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/392702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# building vocabulary\n",
    "vocab = Vocabulary.from_instances(train_dataset_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "trained-findings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['<s>',\n",
       "  'How',\n",
       "  'Ġdo',\n",
       "  'Ġyou',\n",
       "  'Ġknow',\n",
       "  '?',\n",
       "  'ĠAll',\n",
       "  'Ġthis',\n",
       "  'Ġis',\n",
       "  'Ġtheir',\n",
       "  'Ġinformation',\n",
       "  'Ġagain',\n",
       "  '.',\n",
       "  '</s>',\n",
       "  '</s>',\n",
       "  'This',\n",
       "  'Ġinformation',\n",
       "  'Ġbelongs',\n",
       "  'Ġto',\n",
       "  'Ġthem',\n",
       "  '.',\n",
       "  '</s>'],\n",
       " 'label': 'entailment'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_list[3].human_readable_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "gentle-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reader._tokenizer.tokenize(\"ahoo ahoo <mask> this is a book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567c78e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3].idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fcc3540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader._tokenizer.tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-ghost",
   "metadata": {},
   "source": [
    "# Data Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e01e6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-burlington",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "elder-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.basic_classifier import BasicClassifier\n",
    "from allennlp.models.model import Model\n",
    "\n",
    "params = Params.from_file('spurious_corr/MNLI/training_config/mnli_roberta_cpu.jsonnet')\n",
    "\n",
    "# Grab the part of the `config` that defines the model\n",
    "# model_params = params.pop(\"model\")\n",
    "# print(model_params.pop(\"type\"))\n",
    "# print(model_params)\n",
    "# Find out which model subclass we want\n",
    "# model_name = model_params.pop(\"type\")\n",
    "\n",
    "# Instantiate that subclass with the remaining model params\n",
    "#model = Model.from_params(model_params,vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "concerned-madonna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/config.json\n",
      "/ist/users/canu/.cache/huggingface/transformers https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model.load(params, '/ist/users/canu/CI4RRL_project/outputs_real_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e6f206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from copy import deepcopy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "274823cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@@PADDING@@'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab._index_to_token['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8ea32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6acf2bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.instance.Instance at 0x7fdc7f1fc190>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset_list[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c624909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': <allennlp.data.fields.text_field.TextField at 0x7fdc7a18e380>,\n",
       " 'cf_tokens': <allennlp.data.fields.text_field.TextField at 0x7fdc7a18e8c0>,\n",
       " 'label': <allennlp.data.fields.label_field.LabelField at 0x7fdc7a18ec40>}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x = cf_dataset_list[73]\n",
    "new_y = copy.deepcopy( cf_dataset_list[73])\n",
    "\n",
    "new_y.fields['tokens'] = new_y.fields.pop('cf_tokens')\n",
    "new_x.fields.pop('cf_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a07ce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': [-0.7791373133659363, -0.6617544889450073, 0.8475649952888489],\n",
       " 'probs': [0.1386650800704956, 0.15593577921390533, 0.7053991556167603],\n",
       " 'token_ids': [0,\n",
       "  3762,\n",
       "  631,\n",
       "  21,\n",
       "  12648,\n",
       "  162,\n",
       "  24506,\n",
       "  6459,\n",
       "  6,\n",
       "  53,\n",
       "  127,\n",
       "  1144,\n",
       "  851,\n",
       "  10,\n",
       "  372,\n",
       "  45219,\n",
       "  9,\n",
       "  3500,\n",
       "  77,\n",
       "  38,\n",
       "  794,\n",
       "  127,\n",
       "  17589,\n",
       "  3121,\n",
       "  6480,\n",
       "  575,\n",
       "  12445,\n",
       "  81,\n",
       "  5,\n",
       "  124,\n",
       "  9,\n",
       "  10,\n",
       "  3428,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  100,\n",
       "  21,\n",
       "  24506,\n",
       "  6459,\n",
       "  3915,\n",
       "  59,\n",
       "  171,\n",
       "  383,\n",
       "  4,\n",
       "  1437,\n",
       "  2],\n",
       " 'loss': 1.8583110570907593,\n",
       " 'label': 'neutral',\n",
       " 'tokens': ['<s>',\n",
       "  'One',\n",
       "  'Ġthing',\n",
       "  'Ġwas',\n",
       "  'Ġworrying',\n",
       "  'Ġme',\n",
       "  'Ġdread',\n",
       "  'fully',\n",
       "  ',',\n",
       "  'Ġbut',\n",
       "  'Ġmy',\n",
       "  'Ġheart',\n",
       "  'Ġgave',\n",
       "  'Ġa',\n",
       "  'Ġgreat',\n",
       "  'Ġthrob',\n",
       "  'Ġof',\n",
       "  'Ġrelief',\n",
       "  'Ġwhen',\n",
       "  'ĠI',\n",
       "  'Ġsaw',\n",
       "  'Ġmy',\n",
       "  'Ġul',\n",
       "  'ster',\n",
       "  'Ġlying',\n",
       "  'Ġcare',\n",
       "  'lessly',\n",
       "  'Ġover',\n",
       "  'Ġthe',\n",
       "  'Ġback',\n",
       "  'Ġof',\n",
       "  'Ġa',\n",
       "  'Ġchair',\n",
       "  '.',\n",
       "  '</s>',\n",
       "  '</s>',\n",
       "  'I',\n",
       "  'Ġwas',\n",
       "  'Ġdread',\n",
       "  'fully',\n",
       "  'Ġworried',\n",
       "  'Ġabout',\n",
       "  'Ġmany',\n",
       "  'Ġthings',\n",
       "  '.',\n",
       "  'Ġ',\n",
       "  '</s>']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictor.predict_instance(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3497818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<s>, </s>, </s>, </s>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.fields['tokens'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b72b6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset_list[73].fields['label'].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = reader.text_to_instance('','')\n",
    "out_x = predictor.predict_instance(new_x)['logits']\n",
    "count_correct_normie = 0\n",
    "count_correct_ci = 0\n",
    "label_dict = {0:'entailment',1:'contradiction',2:'neutral'}\n",
    "for i in range(len(validation_dataset_list)):\n",
    "    test_x_1 = validation_dataset_list[i]\n",
    "    y_true  = test_x_1.fields['label'].label\n",
    "    out_normie = predictor.predict_instance(test_x_1)\n",
    "    y_pred_normie = out_normie['label']\n",
    "    if y_true == y_pred_normie:\n",
    "        count_correct_normie+=1\n",
    "    out = np.array(out_normie['logits']) - 0.5*np.array(out_x)\n",
    "    y_pred_ci = label_dict[np.argmax(softmax(out))]\n",
    "    if y_true == y_pred_ci:\n",
    "        count_correct_ci+=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93764b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(validation_dataset_list)\n",
    "print(count_correct_normie/n, count_correct_ci/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99f03b",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d116e1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader.Params({'batch_sampler': {'batch_size': 32, 'type': 'bucket'}})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdc4c132fb9481ebc842bf0304ec7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50f259f1c5541a7ac23de18201c09f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd77ff5560714f7c88de283d74410a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from allennlp.data import DataLoader\n",
    "\n",
    "params = Params.from_file('spurious_corr/MNLI/training_config/mnli_roberta_cpu.jsonnet')\n",
    "# Grab the part of the `config` that defines the \n",
    "dataloader_params = params.pop(\"data_loader\")\n",
    "print(dataloader_params)\n",
    "# Instantiate that subclass with the remaining model params\n",
    "# train_dataset.index_with(vocab)\n",
    "# validation_dataset.index_with(vocab)\n",
    "data_loader = DataLoader.from_params(\n",
    "            params=dataloader_params, reader=reader, data_path=train_data_path\n",
    "        )\n",
    "\n",
    "\n",
    "params = Params.from_file('spurious_corr/MNLI/training_config/mnli_roberta_cpu.jsonnet')\n",
    "# Grab the part of the `config` that defines the model\n",
    "dataloader_params = params.pop(\"data_loader\")\n",
    "validation_data_loader = DataLoader.from_params(\n",
    "            params=dataloader_params, reader=reader, data_path=validation_data_path\n",
    "        )\n",
    "\n",
    "params = Params.from_file('spurious_corr/MNLI/training_config/mnli_roberta_cpu.jsonnet')\n",
    "# Grab the part of the `config` that defines the model\n",
    "dataloader_params = params.pop(\"data_loader\")\n",
    "cf_data_loader = DataLoader.from_params(\n",
    "            params=dataloader_params, reader=cf_reader, data_path=validation_data_path\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a068f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.set_target_device(-1)\n",
    "data_loader.index_with(model.vocab)\n",
    "\n",
    "# adversarial_data_loader.set_target_device(0)\n",
    "# adversarial_data_loader.index_with(vocab)\n",
    "\n",
    "validation_data_loader.set_target_device(-1)\n",
    "validation_data_loader.index_with(model.vocab)\n",
    "\n",
    "cf_data_loader.set_target_device(-1)\n",
    "cf_data_loader.index_with(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea1acd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens___token_ids': 79,\n",
       "  'tokens___mask': 79,\n",
       "  'tokens___type_ids': 79,\n",
       "  'tokens___segment_concat_mask': 79},\n",
       " 'cf_tokens': {'tokens___token_ids': 4,\n",
       "  'tokens___mask': 4,\n",
       "  'tokens___type_ids': 4,\n",
       "  'tokens___segment_concat_mask': 4},\n",
       " 'label': {}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cf_data_loader.iter_instances())[2204].get_padding_lengths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745e790",
   "metadata": {},
   "source": [
    "# Counterfactual Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f048e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from os import PathLike\n",
    "from typing import Any, Dict, Iterable, Optional, Union, Tuple, Set, List\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from allennlp.common.checks import check_for_gpu, ConfigurationError\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.common.util import dump_metrics, sanitize, int_to_device\n",
    "from allennlp.data import Instance, Vocabulary, Batch, DataLoader\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.models.archival import CONFIG_NAME\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "# We want to warn people that tqdm ignores metrics that start with underscores\n",
    "# exactly once. This variable keeps track of whether we have.\n",
    "class HasBeenWarned:\n",
    "    tqdm_ignores_underscores = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d4e2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "def cf_evaluate(\n",
    "    model: Model,\n",
    "    data_loader: DataLoader,\n",
    "    cf_weight: float = 0.5,\n",
    "    cuda_device: int = -1,\n",
    "    batch_weight_key: str = None,\n",
    "    output_file: str = None,\n",
    "    predictions_output_file: str = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    model : `Model`\n",
    "        The model to evaluate\n",
    "    data_loader : `DataLoader`\n",
    "        The `DataLoader` that will iterate over the evaluation data (data loaders already contain\n",
    "        their data).\n",
    "    cuda_device : `int`, optional (default=`-1`)\n",
    "        The cuda device to use for this evaluation.  The model is assumed to already be using this\n",
    "        device; this parameter is only used for moving the input data to the correct device.\n",
    "    batch_weight_key : `str`, optional (default=`None`)\n",
    "        If given, this is a key in the output dictionary for each batch that specifies how to weight\n",
    "        the loss for that batch.  If this is not given, we use a weight of 1 for every batch.\n",
    "    metrics_output_file : `str`, optional (default=`None`)\n",
    "        Optional path to write the final metrics to.\n",
    "    predictions_output_file : `str`, optional (default=`None`)\n",
    "        Optional path to write the predictions to.\n",
    "    # Returns\n",
    "    `Dict[str, Any]`\n",
    "        The final metrics.\n",
    "    \"\"\"\n",
    "    check_for_gpu(cuda_device)\n",
    "    data_loader.set_target_device(int_to_device(cuda_device))\n",
    "    predictions_file = (\n",
    "        None if predictions_output_file is None else open(predictions_output_file, \"w\")\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        iterator = iter(data_loader)\n",
    "        logger.info(\"Iterating over dataset\")\n",
    "        generator_tqdm = Tqdm.tqdm(iterator)\n",
    "\n",
    "        # Number of batches in instances.\n",
    "        batch_count = 0\n",
    "        # Number of batches where the model produces a loss.\n",
    "        loss_count = 0\n",
    "        # Cumulative weighted loss\n",
    "        total_loss = 0.0\n",
    "        # Cumulative weight across all batches.\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for batch in generator_tqdm:\n",
    "            # create cf batch\n",
    "            batch_cf = copy.deepcopy(batch)\n",
    "            batch_cf['tokens'] = batch_cf.pop('cf_tokens')\n",
    "            batch.pop('cf_tokens')\n",
    "            \n",
    "            batch_count += 1\n",
    "            batch = nn_util.move_to_device(batch, cuda_device)\n",
    "            output_dict = model(**batch)\n",
    "            output_dict_cf = model(**batch_cf)\n",
    "            output_dict['logits'] = output_dict['logits']-cf_wright*output_dict_cf['logits']\n",
    "            probs = torch.nn.functional.softmax(output_dict['logits'], dim=-1)\n",
    "           \n",
    "            output_dict['probs'] = probs\n",
    "            loss = output_dict.get(\"loss\")\n",
    "\n",
    "            metrics = model.get_metrics()\n",
    "\n",
    "            if loss is not None:\n",
    "                loss_count += 1\n",
    "                if batch_weight_key:\n",
    "                    weight = output_dict[batch_weight_key].item()\n",
    "                else:\n",
    "                    weight = 1.0\n",
    "\n",
    "                total_weight += weight\n",
    "                total_loss += loss.item() * weight\n",
    "                # Report the average loss so far.\n",
    "                metrics[\"loss\"] = total_loss / total_weight\n",
    "\n",
    "            if not HasBeenWarned.tqdm_ignores_underscores and any(\n",
    "                metric_name.startswith(\"_\") for metric_name in metrics\n",
    "            ):\n",
    "                logger.warning(\n",
    "                    'Metrics with names beginning with \"_\" will '\n",
    "                    \"not be logged to the tqdm progress bar.\"\n",
    "                )\n",
    "                HasBeenWarned.tqdm_ignores_underscores = True\n",
    "            description = (\n",
    "                \", \".join(\n",
    "                    [\n",
    "                        \"%s: %.2f\" % (name, value)\n",
    "                        for name, value in metrics.items()\n",
    "                        if not name.startswith(\"_\")\n",
    "                    ]\n",
    "                )\n",
    "                + \" ||\"\n",
    "            )\n",
    "            generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "            if predictions_file is not None:\n",
    "                predictions = json.dumps(sanitize(model.make_output_human_readable(output_dict)))\n",
    "                predictions_file.write(predictions + \"\\n\")\n",
    "\n",
    "        if predictions_file is not None:\n",
    "            predictions_file.close()\n",
    "        \n",
    "        # recaculate accuracy\n",
    "        model._accuracy(output_dict['logits'], batch['label'])\n",
    "        final_metrics = model.get_metrics(reset=True)\n",
    "        if loss_count > 0:\n",
    "            # Sanity check\n",
    "            if loss_count != batch_count:\n",
    "                raise RuntimeError(\n",
    "                    \"The model you are trying to evaluate only sometimes produced a loss!\"\n",
    "                )\n",
    "            final_metrics[\"loss\"] = total_loss / total_weight\n",
    "            final_metrics[\"loss\"] = \"N/A\"\n",
    "        if output_file is not None:\n",
    "            dump_metrics(output_file, final_metrics, log=True)\n",
    "\n",
    "        return final_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5af4d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb2ae7b7fda4be3be063f90932fe5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_metrics = cf_evaluate(model,cf_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5d930d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6301725454267827, 'loss': 'N/A'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81b1d046",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_batch_indices() missing 1 required positional argument: 'instances'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114897/4282104566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_data_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_batch_indices() missing 1 required positional argument: 'instances'"
     ]
    }
   ],
   "source": [
    "validation_data_loader.batch_sampler.get_batch_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace allennlp predict to create new evaluate code\n",
    "new_x = reader.text_to_instance('','')\n",
    "xtemp  = reader.apply_token_indexers(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6360a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x.index_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset_list[0].fields['tokens'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cefe42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = validation_dataset_list[2]\n",
    "out_normie = predictor.predict_instance(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba771fe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114897/3721036092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classification_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token_ids_from_text_field_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "logits = self._classification_layer(embedded_text)\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "output_dict = {\"logits\": logits, \"probs\": probs}\n",
    "output_dict[\"token_ids\"] = util.get_token_ids_from_text_field_tensors(tokens)\n",
    "if label is not None:\n",
    "    loss = self._loss(logits, label.long().view(-1))\n",
    "    output_dict[\"loss\"] = loss\n",
    "    self._accuracy(logits, label)\n",
    "\n",
    "return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c84f75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2f6b457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "Default implementation multiprocess is not registered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114897/2995374217.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_data_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CI_env/lib/python3.8/site-packages/allennlp/common/registrable.py\u001b[0m in \u001b[0;36mlist_available\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Default implementation {default} is not registered\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: Default implementation multiprocess is not registered"
     ]
    }
   ],
   "source": [
    "validation_data_loader.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71221b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
