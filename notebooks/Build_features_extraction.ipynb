{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3db8cd9",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d82515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb90842",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/nli/multinli_1.0_train.jsonl\"\n",
    "data = pd.read_json(data_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_count = {}\n",
    "for i in range(len(data)):\n",
    "  if data['gold_label'][i] not in to_count:\n",
    "    to_count[data['gold_label'][i]] = 1\n",
    "  else:\n",
    "    to_count[data['gold_label'][i]] += 1\n",
    "to_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_c = 0\n",
    "for i in range(len(data)):\n",
    "  if data['gold_label'][i] == '-':\n",
    "    _c += 1\n",
    "_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_and_label = []\n",
    "for i in range(len(data)):\n",
    "    pair_and_label.append((data['sentence1'][i], data['sentence2'][i], data['gold_label'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e499d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pair_and_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5df484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pair_label'] = pair_and_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39110493",
   "metadata": {},
   "source": [
    "# Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d63f3e",
   "metadata": {},
   "source": [
    "## Lexical Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17671309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_overlap(pair_label):\n",
    "    premise = pair_label[0]\n",
    "    hypothesis = pair_label[1]\n",
    "    label = pair_label[2]\n",
    "    #all = {\"neutral\": [], \"contradiction\": [], \"entailment\": [], \"else\": []} \n",
    "\n",
    "    prem_words = []\n",
    "    hyp_words = []\n",
    "\n",
    "    for word in premise.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            prem_words.append(word.lower())\n",
    "\n",
    "    for word in hypothesis.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            hyp_words.append(word.lower())\n",
    "\n",
    "    prem_filtered = \" \".join(prem_words)\n",
    "    hyp_filtered = \" \".join(hyp_words)\n",
    "\n",
    "    count = 0\n",
    "    for word in hyp_words:\n",
    "        if word in prem_words:\n",
    "            count += 1\n",
    "\n",
    "    if count >= len(hyp_words)*80/100:\n",
    "        all_in = True\n",
    "    else:\n",
    "        all_in = False\n",
    "\n",
    "    if all_in:\n",
    "        if premise == 'entailment':\n",
    "            return 'easy'\n",
    "        elif premise == 'neutral':\n",
    "            return 'hard'\n",
    "        elif premise == 'contradiction':\n",
    "            return 'hard'\n",
    "        else:\n",
    "            return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_overlap(data['pair_label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lexical_overlap'] = data['pair_label'].apply(lexical_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc605e",
   "metadata": {},
   "source": [
    "## Word Swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_count(pair_label):\n",
    "    premise = pair_label[0]\n",
    "    hypothesis = pair_label[1]\n",
    "    label = pair_label[2]\n",
    "\n",
    "    all = {\"neutral\": [], \"contradiction\": [], \"entailment\": [], \"else\": []} \n",
    "    twen = []\n",
    "    prem_words = []\n",
    "    hyp_words = []\n",
    "\n",
    "    for word in premise.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            prem_words.append(word.lower())\n",
    "\n",
    "    for word in hypothesis.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            hyp_words.append(word.lower())\n",
    "\n",
    "    prem_filtered = \" \".join(prem_words)\n",
    "    hyp_filtered = \" \".join(hyp_words)\n",
    "\n",
    "    count = 0\n",
    "    for word in hyp_words:\n",
    "        if word in prem_words:\n",
    "            count += 1\n",
    "\n",
    "    if count > len(hyp_words)*0/100:\n",
    "        all_in = True\n",
    "    else:\n",
    "        all_in = False\n",
    "\n",
    "    if all_in:\n",
    "        twen.append((premise, hypothesis))\n",
    "\n",
    "    return twen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb89264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_swapping(prem_and_hypo):\n",
    "    new_prem, new_hypo = '', ''\n",
    "    prem = prem_and_hypo[0].replace(\"  \", \" \")\n",
    "    hypo = prem_and_hypo[1].replace(\"  \", \" \")\n",
    "    tokens_prem = prem.lower().split()\n",
    "    tokens_hypo = hypo.lower().split()\n",
    "\n",
    "    intersect = list(set(tokens_prem).intersection(tokens_hypo))\n",
    "\n",
    "    for i in range(len(tokens_prem)): \n",
    "        if tokens_prem[i] in intersect:\n",
    "            new_prem += tokens_prem[i]+' '\n",
    "    for i in range(len(tokens_hypo)):\n",
    "        if tokens_hypo[i] in intersect:\n",
    "            new_hypo += tokens_hypo[i]+' '\n",
    "\n",
    "    if len(new_prem.split()) == 1 and len(new_hypo.split()) == 1:\n",
    "        return \"-\"\n",
    "    elif len(new_prem.split()) == 0 and len(new_hypo.split()) == 0:\n",
    "        return \"-\"\n",
    "\n",
    "    if len(new_prem.strip()) > len(new_hypo.strip()):\n",
    "        dif = len(new_prem.strip())-len(new_hypo.strip())\n",
    "        distance = Levenshtein.distance(new_prem.strip(), new_hypo.strip())\n",
    "        if dif == distance:\n",
    "            return \"Not Swap\"\n",
    "        else:\n",
    "            return \"Swap\"\n",
    "    else:\n",
    "        dif = len(new_hypo.strip())-len(new_prem.strip())\n",
    "        distance = Levenshtein.distance(new_prem.strip(), new_hypo.strip())\n",
    "        if dif == distance:\n",
    "            return \"Not Swap\"\n",
    "        else:\n",
    "            return \"Swap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_swapping((\"Carl Newton and I wrote a letter\", \"Carl wrote a letter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap, not_swap, dash = 0, 0, 0\n",
    "for i in data['pair_label']:\n",
    "    res = detect_swapping(i)\n",
    "    if res == \"Swap\":\n",
    "        swap += 1\n",
    "    elif res == \"Not Swap\":\n",
    "        not_swap += 1\n",
    "    else:\n",
    "        dash += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_swapping'] = data['pair_label'].apply(detect_swapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a518eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d72d51",
   "metadata": {},
   "source": [
    "## Hypothesis Lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d73e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_len(pair_label):\n",
    "    hypo_tokens = pair_label[1].strip().split()\n",
    "    return len(hypo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5873e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hypothesis length'] = data['pair_label'].apply(hypo_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([data['hypothesis length']])\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af29bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = np.array([data['hypothesis length']]).reshape(-1, 1)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scale = scaler.fit_transform(length)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23714032",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = []\n",
    "for i in range(len(scale)):\n",
    "    scale_list.append(scale[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b679c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hypo_len'] = scale_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"hypothesis length\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b043c",
   "metadata": {},
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e366bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_negation(pair_label):\n",
    "    premise = pair_label[0]\n",
    "    hypothesis = pair_label[1]\n",
    "    label = pair_label[2]\n",
    "\n",
    "    keywords = [\" not \", \" no \", \"n't\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"cannot\", \" nor \"]\n",
    "    count = 0\n",
    "    for key in keywords:\n",
    "    if key in premise or key in hypothesis:\n",
    "          count += 1\n",
    "    # elif key not in premise and key not in hypothesis:\n",
    "        # none += 1\n",
    "\n",
    "    if count > 0:\n",
    "        if label.strip() != 'contradiction':\n",
    "            return 'hard'\n",
    "        else:\n",
    "            return 'easy'\n",
    "    elif count == 0:\n",
    "        return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "have_negation((\"I do miss you\", \"I do not miss you\", \"neutral\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['negation'] = data['pair_label'].apply(have_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7840a0a",
   "metadata": {},
   "source": [
    "## Subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subseq(pair_label):\n",
    "    premise = pair_label[0]\n",
    "    hypothesis = pair_label[1]\n",
    "    label = pair_label[2]\n",
    "\n",
    "    prem_words = []\n",
    "    hyp_words = []\n",
    "\n",
    "    for word in premise.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            prem_words.append(word.lower())\n",
    "\n",
    "    for word in hypothesis.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            hyp_words.append(word.lower())\n",
    "\n",
    "    prem_filtered = \" \".join(prem_words)\n",
    "    hyp_filtered = \" \".join(hyp_words)\n",
    "\n",
    "    if hyp_filtered in prem_filtered:\n",
    "        if label == 'entailment':\n",
    "            return 'easy'\n",
    "        else:\n",
    "            return 'hard'\n",
    "    else:\n",
    "        return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subsequence'] = data['pair_label'].apply(subseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d88895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6662e",
   "metadata": {},
   "source": [
    "## Constituent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = open(\"../data/nli/multinli_1.0_train.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_phrase_list(parse, phrases):\n",
    "    #print(parse)\n",
    "    if parse == \"\":\n",
    "        return phrases\n",
    "    \n",
    "    phrase_list = phrases\n",
    "\n",
    "    words = parse.split()\n",
    "    this_phrase = []\n",
    "    next_level_parse = []\n",
    "    for index, word in enumerate(words):\n",
    "        if word == \"(\":\n",
    "            next_level_parse += this_phrase\n",
    "            this_phrase = [\"(\"]\n",
    "\n",
    "        elif word == \")\" and len(this_phrase) > 0 and this_phrase[0] == \"(\":\n",
    "            phrase_list.append(\" \".join(this_phrase[1:]))\n",
    "            next_level_parse += this_phrase[1:]\n",
    "            this_phrase = []\n",
    "        elif word == \")\":\n",
    "            next_level_parse += this_phrase\n",
    "            next_level_parse.append(\")\")\n",
    "            this_phrase = []\n",
    "        else:\n",
    "            this_phrase.append(word)\n",
    "\n",
    "    return parse_phrase_list(\" \".join(next_level_parse), phrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe452f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent = []\n",
    "count_entailment = 0\n",
    "count_neutral = 0\n",
    "count_contradiction = 0\n",
    "first = True\n",
    "counter = 0\n",
    "for line in fi:\n",
    "    counter += 1\n",
    "\n",
    "    if first:\n",
    "        first = False\n",
    "        continue\n",
    "\n",
    "\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "\n",
    "    premise = parts[5]\n",
    "    hypothesis = parts[6]\n",
    "    label = parts[0]\n",
    "    parse = parts[1]\n",
    "\n",
    "    parse_new = []\n",
    "    for word in parse.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            parse_new.append(word.lower())\n",
    "\n",
    "    all_phrases = parse_phrase_list(\" \".join(parse_new), [])\n",
    "\n",
    "    prem_words = []\n",
    "    hyp_words = []\n",
    "\n",
    "    for word in premise.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            prem_words.append(word.lower().replace(\".\", \"\").replace(\"?\", \"\").replace(\"!\", \"\"))\n",
    "\n",
    "    for word in hypothesis.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            hyp_words.append(word.lower().replace(\".\", \"\").replace(\"?\", \"\").replace(\"!\", \"\"))\n",
    "\n",
    "    prem_filtered = \" \".join(prem_words)\n",
    "    hyp_filtered = \" \".join(hyp_words)\n",
    "\n",
    "    if hyp_filtered in all_phrases:\n",
    "        if label == \"entailment\":\n",
    "            constituent.append((premise, hypothesis, label))\n",
    "        if label == \"neutral\":\n",
    "            constituent.append((premise, hypothesis, label))\n",
    "        if label == \"contradiction\":\n",
    "            constituent.append((premise, hypothesis, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons(pair_label):\n",
    "    if pair_label in constituent:\n",
    "        if pair_label[2] == 'entailment':\n",
    "            return 'easy'\n",
    "        else:\n",
    "            return 'hard'\n",
    "    else:\n",
    "        return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['constituent'] = data['pair_label'].apply(cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62dd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1082e5c",
   "metadata": {},
   "source": [
    "## Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac25306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10cfd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe01c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_antonyms(string):\n",
    "    successful = 0\n",
    "    error404 = 0\n",
    "    etc = 0\n",
    "    try:\n",
    "        # Remove whitespace before and after word and use underscore between words\n",
    "        stripped_string = string.strip()\n",
    "        fixed_string = stripped_string.replace(\" \", \"_\")\n",
    "\n",
    "        # Set the url using the amended string\n",
    "        my_url = f'https://thesaurus.plus/thesaurus/{fixed_string}'\n",
    "\n",
    "        res = requests.get(my_url)\n",
    "        res.encoding = \"utf-8\"\n",
    "\n",
    "        if res.status_code == 200:\n",
    "            successful += 1\n",
    "#             print(\"Successful\")\n",
    "        elif res.status_code == 404:\n",
    "            error404 += 1\n",
    "#             print(\"Error 404 page not found\")\n",
    "        else:\n",
    "            etc += 1\n",
    "#             print(\"Not both 200 and 404\")\n",
    "\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        nyms = soup.find_all(\"ul\", {\"class\": \"list paper\"})\n",
    "\n",
    "        output = [[], []]\n",
    "\n",
    "        for idx, n in enumerate(nyms):\n",
    "            a = n.find_all(\"div\", \"list_item\")\n",
    "            for b in a:\n",
    "                output[idx].append(b.text.strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    antonym = output[1]\n",
    "    \n",
    "    return antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_antonyms = {}\n",
    "for i in range(len(word_overlap)):\n",
    "    tokens = word_overlap['sentence1'][i].split()\n",
    "    for token in tokens:\n",
    "        token = token.replace(\".\", \"\")\n",
    "        token = token.replace(\",\", \"\")\n",
    "        token = token.replace(\"-\", \" \")\n",
    "        token = token.replace(\"'s\", \"\")\n",
    "        token = token.lower().strip()\n",
    "        if token not in all_antonyms and token not in stop_words and \" \" not in token and \"\\\"\" not in token and \"?\" not in token and not token.isdigit() and \"(\" not in token and \")\" not in token and \"/\" not in token and \"'\" not in token and \"$\" not in token and \";\" not in token and \":\" not in token and \"[\" not in token and \"]\" not in token:\n",
    "            lemma_token = lemmatizer.lemmatize(token)\n",
    "            all_antonyms[lemma_token] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "antonymy = []\n",
    "for key in all_antonyms:\n",
    "    antonymy.append(key)\n",
    "len(antonymy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "every_antonym = {}\n",
    "for word in tqdm(antonymy):\n",
    "    every_antonym[word] = []\n",
    "    # print(count, word)\n",
    "    result = find_antonyms(word.strip())\n",
    "    if len(result) == 0:\n",
    "        count += 1\n",
    "        continue\n",
    "    else:\n",
    "        # print(result)\n",
    "        for each in result:\n",
    "            every_antonym[word].append(each)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Successful: {successful}')\n",
    "print(f'Error 404 page not foun: {error404}')\n",
    "print(f'Not both 200 and 404: {etc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym(pair_label):\n",
    "    premise = pair_label[0]\n",
    "    hypothesis = pair_label[1]\n",
    "    label = pair_label[2]\n",
    "\n",
    "    premise_tokens = premise.split()\n",
    "    hypo_tokens = hypothesis.split()\n",
    "\n",
    "    count = 0\n",
    "    for token in premise_tokens:\n",
    "        if token in all_antonyms:\n",
    "            for ant in all_antonyms[token]:\n",
    "                if ant in hypo_tokens:\n",
    "                    count += 1\n",
    "  \n",
    "    if count > 0:\n",
    "        if label == 'contradiction':\n",
    "            return 'easy'\n",
    "        else:\n",
    "            return 'hard'\n",
    "    else:\n",
    "        return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['antonym'] = data['pair_label'].apply(antonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c757279",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a134429",
   "metadata": {},
   "source": [
    "## Overlapping Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023347b",
   "metadata": {},
   "source": [
    "### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3dae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = \"[.,!?]\"\n",
    "word_overlap['sentence1'] = word_overlap['sentence1'].str.replace(pat, \"\", regex=False)\n",
    "word_overlap['sentence2'] = word_overlap['sentence2'].str.replace(pat, \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi = {\"entailment\": {}, \"neutral\": {}, \"contradiction\": {}}\n",
    "for i in range(len(data)):\n",
    "#     print(i)\n",
    "    prem = data['sentence1'][i].lower().split()\n",
    "    hypo = data['sentence2'][i].lower().split()\n",
    "\n",
    "    if data['gold_label'][i] == \"entailment\":\n",
    "        for token in prem:\n",
    "            if token not in pmi['entailment']:\n",
    "                pmi['entailment'][token] = 1\n",
    "            else:\n",
    "                pmi['entailment'][token] += 1\n",
    "        for token in hypo:\n",
    "            if token not in pmi['entailment']:\n",
    "                pmi['entailment'][token] = 1\n",
    "            else:\n",
    "                pmi['entailment'][token] += 1\n",
    "    elif data['gold_label'][i] == \"neutral\":\n",
    "        for token in prem:\n",
    "            if token not in pmi['neutral']:\n",
    "                pmi['neutral'][token] = 1\n",
    "            else:\n",
    "                pmi['neutral'][token] += 1\n",
    "        for token in hypo:\n",
    "            if token not in pmi['neutral']:\n",
    "                pmi['neutral'][token] = 1\n",
    "            else:\n",
    "                pmi['neutral'][token] += 1\n",
    "    else:\n",
    "        for token in prem:\n",
    "            if token not in pmi['contradiction']:\n",
    "                pmi['contradiction'][token] = 1\n",
    "            else:\n",
    "                pmi['contradiction'][token] += 1\n",
    "        for token in hypo:\n",
    "            if token not in pmi['contradiction']:\n",
    "                pmi['contradiction'][token] = 1\n",
    "            else:\n",
    "                pmi['contradiction'][token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = {}\n",
    "for i in range(len(data)):\n",
    "    print(i)\n",
    "    prem = data['sentence1'][i].lower().split()\n",
    "    hypo = data['sentence2'][i].lower().split()\n",
    "    for token in prem:\n",
    "        if token not in total:\n",
    "            total[token] = 1\n",
    "        else:\n",
    "            total[token] += 1\n",
    "    for token in hypo:\n",
    "        if token not in total:\n",
    "            total[token] = 1\n",
    "        else:\n",
    "            total[token] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "entail_word, con_word, neu_word = 0, 0, 0\n",
    "for num in pmi['entailment']:\n",
    "    entail_word += pmi['entailment'][num]\n",
    "\n",
    "for num in pmi['contradiction']:\n",
    "    con_word += pmi['contradiction'][num]\n",
    "\n",
    "for num in pmi['neutral']:\n",
    "    neu_word += pmi['neutral'][num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'entialment: {entail_word}')\n",
    "print(f'contradiction: {con_word}')\n",
    "print(f'neutral: {neu_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2090e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi_entail(word):\n",
    "    prob_word_class = pmi['entailment'][word]/float(all)\n",
    "    prob_word = total[word]/float(all)\n",
    "    prob_class = entail_word/float(all)\n",
    "    result = max(np.log(prob_word_class/(prob_word*prob_class)), 0.0)\n",
    "    return result\n",
    "\n",
    "def pmi_con(word):\n",
    "    prob_word_class = pmi['contradiction'][word]/float(all)\n",
    "    prob_word = total[word]/float(all)\n",
    "    prob_class = con_word/float(all)\n",
    "    result = max(np.log(prob_word_class/(prob_word*prob_class)), 0.0)\n",
    "    return result\n",
    "\n",
    "def pmi_neu(word):\n",
    "    prob_word_class = pmi['neutral'][word]/float(all)\n",
    "    prob_word = total[word]/float(all)\n",
    "    prob_class = neu_word/float(all)\n",
    "    result = max(np.log(prob_word_class/(prob_word*prob_class)), 0.0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_pmi = {}\n",
    "for word in pmi[\"entailment\"]:\n",
    "    if pmi['entailment'][word] > 500:\n",
    "        entailment_pmi[word] = pmi_entail(word)\n",
    "\n",
    "sort_pmi_entail = sorted(entailment_pmi.items(), key=lambda x: x[1], reverse=True)\n",
    "sort_pmi_entail[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "contradiction_pmi = {}\n",
    "for word in pmi[\"contradiction\"]:\n",
    "    if pmi['contradiction'][word] > 500:\n",
    "    contradiction_pmi[word] = pmi_con(word)\n",
    "\n",
    "sort_pmi_cons = sorted(contradiction_pmi.items(), key=lambda x: x[1], reverse=True)\n",
    "sort_pmi_cons[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_pmi = {}\n",
    "for word in pmi[\"neutral\"]:\n",
    "    if pmi['neutral'][word] > 500:\n",
    "        neutral_pmi[word] = pmi_neu(word)\n",
    "    \n",
    "sort_pmi_neutral = sorted(neutral_pmi.items(), key=lambda x: x[1], reverse=True)\n",
    "sort_pmi_neutral[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9db213",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"\\d+\")\n",
    "pmi_neutral_dict = {\"neutral\": []}\n",
    "count_neu = 0\n",
    "for i in sort_pmi_neutral[:50]:\n",
    "    count_neu += 1\n",
    "    result = re.match(pattern, i[0])\n",
    "    if not result:\n",
    "        pmi_neutral_dict['neutral'].append(i[0])\n",
    "    if len(pmi_neutral_dict['neutral']) == 40:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_neutral_dict['neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c39f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_ent_dict = {\"entailment\": []}\n",
    "for i in sort_pmi_entail[:40]:\n",
    "    pmi_ent_dict['entailment'].append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_cons_dict = {\"contradiction\": []}\n",
    "for i in sort_pmi_cons[:40]:\n",
    "    pmi_cons_dict['contradiction'].append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ecfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_df = pd.DataFrame(pmi_neutral_dict)\n",
    "pmi_df[\"entailment\"] = pmi_ent_dict[\"entailment\"]\n",
    "pmi_df[\"contradiction\"] = pmi_cons_dict[\"contradiction\"]\n",
    "pmi_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c368339",
   "metadata": {},
   "source": [
    "### Make bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = []\n",
    "bow.append(pmi_df.neutral.values.tolist())\n",
    "bow.append(pmi_df.entailment.values.tolist())\n",
    "bow.append(pmi_df.contradiction.values.tolist())\n",
    "\n",
    "bow1d = [item for nest in bow for item in nest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a719a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in bow1d:\n",
    "    def make_bow(pair_label):\n",
    "        count = 0\n",
    "        prem = pair_label[0]\n",
    "        hypo = pair_label[1]\n",
    "        combine = prem+\" \"+hypo\n",
    "        tokens = combine.lower().split()\n",
    "            for token in tokens:\n",
    "                if token.strip() == b:\n",
    "                count += 1\n",
    "            if count > 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        data[b] = data['pair_label'].apply(make_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae6688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap_score(pair_label):\n",
    "    prem_words = []\n",
    "    hyp_words = []\n",
    "\n",
    "    premise = pair_label[0].strip()\n",
    "    hypothesis = pair_label[1].strip()\n",
    "    gold_label = pair_label[2].strip()\n",
    "\n",
    "    for word in premise.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            prem_words.append(word.lower())\n",
    "\n",
    "    for word in hypothesis.split():\n",
    "        if word not in [\".\", \"?\", \"!\"]:\n",
    "            hyp_words.append(word.lower())\n",
    "\n",
    "    prem_filtered = \" \".join(prem_words)\n",
    "    hyp_filtered = \" \".join(hyp_words)\n",
    "\n",
    "    count = 0\n",
    "    for word in hyp_words:\n",
    "        if word in prem_words:\n",
    "            count+=1\n",
    "\n",
    "    overlap_score = count/len(hyp_words)        \n",
    "    return overlap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24716ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['overlapping score'] = data['pair_label'].apply(get_overlap_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../data/nli/multinli_1.0_train_features_path.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
